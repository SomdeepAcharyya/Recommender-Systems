{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised Personality Recognition for Social Network Sites ",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15AWfUj8J0FUVgwzNO-7D5S8La0XcIYk3",
      "authorship_tag": "ABX9TyP4ujGVHdAWMBk/3vfDbzLF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomdeepAcharyya/Recommender-Systems/blob/main/Unsupervised_Personality_Recognition_for_Social_Network_Sites.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsupervised Personality Recognition for Social Network Sites\n",
        "# Fabio Celli\n",
        "# generation of personality score saccording to formula given by Mairesse features"
      ],
      "metadata": {
        "id": "j-MxyZTpekqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gathered punctutaion, commas, pronouns, negative words, numbers, parenthesis and other features from text | TripAdvisor Dataset \n",
        "# gathered other features from text | TripAdvisor Dataset "
      ],
      "metadata": {
        "id": "OikCdG67e2sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy, cmake\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "hsAC21RtN1Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHpKYvo-LbqE",
        "outputId": "13b4afbb-9900-4fcb-e111-bb8770203070"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'userId', 'type', 'date', 'title', 'text', 'rating',\n",
              "       'helpfulness', 'total_points', 'itemId', 'taObjectUrl', 'taObjectCity',\n",
              "       'open', 'cons', 'extra', 'agree', 'neuro', 'processed_text',\n",
              "       'processed_title', 'ageRange', 'location', 'gender', 'country', 'age',\n",
              "       'sex', 'clusters'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Tripadvisor review Dataset\n",
        "\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/tripadvisor_reviews_with_country.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  tr = pd.read_csv(infile)\n",
        "tr = tr.rename(columns={\"username\":\"userId\", \"taObject\":\"itemId\"})\n",
        "tr.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tr['text']\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PccduQ52N2VO",
        "outputId": "c8f10ebf-e9a7-45d1-c831-c6f09bd2234a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Decent Hotel next to station so good location ...\n",
              "1        Excellent Hotel - well situated for getting ro...\n",
              "2        Great Museum - abslutely worth making the time...\n",
              "3        Stayed for 3 nights in MIami - stayed here due...\n",
              "4        I always visit Hunters when in San Diego. The ...\n",
              "                               ...                        \n",
              "32575    This restaurant is in a prime location in the ...\n",
              "32576    This is a very good caf? on the main square in...\n",
              "32577    Friendly and helpful staff. They helped us cho...\n",
              "32578    We stayed in the studio apartment, which compr...\n",
              "32579    The restaurant offers good quality food at pri...\n",
              "Name: text, Length: 32580, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count punctutaion and commas\n",
        "import string\n",
        "ap = []\n",
        "cm = []\n",
        "du = []\n",
        "em = []\n",
        "for i in x:\n",
        "  p = 0\n",
        "  c = 0\n",
        "  r = 0\n",
        "  ex = 0\n",
        "  for j in i:\n",
        "    if j in string.punctuation:  \n",
        "      p = p + 1\n",
        "    if j == ',':\n",
        "      c = c + 1\n",
        "    if j == '@':\n",
        "      r = r + 1\n",
        "    if j == '!':\n",
        "      ex = ex + 1\n",
        "  ap.append(p)\n",
        "  cm.append(c)\n",
        "  du.append(c)\n",
        "  em.append(ex)"
      ],
      "metadata": {
        "id": "NrEIKDGrPmY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "tokens = []\n",
        "for i in x:\n",
        "  tokens.append(word_tokenize(i))\n",
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "metadata": {
        "id": "w4f5r5jsQY7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count pronouns, negative words, numbers, parenthesis\n",
        "im = []\n",
        "np = []\n",
        "neg_words=['no','not','n\\'t']\n",
        "nb = []\n",
        "pa = []\n",
        "qm = []\n",
        "sl = []\n",
        "sr = []\n",
        "we = []\n",
        "yu = []\n",
        "for i in tokens:\n",
        "  p = 0\n",
        "  n = 0\n",
        "  d = 0\n",
        "  pa = 0\n",
        "  q = 0\n",
        "  l = 0\n",
        "  ps = 0\n",
        "  pp = 0\n",
        "  p2 = 0\n",
        "  for j in i:\n",
        "    if j == 'I' or j == 'me' or j == 'i' or j == 'Me' or j =='My' or j == 'my' or j =='mine' or j =='Mine':\n",
        "      p = p+1\n",
        "    if j in neg_words:\n",
        "      n = n+1\n",
        "    if j.isdigit() is True:\n",
        "      d = d + 1\n",
        "    if j in [\"(\", \")\", \"{\", \"}\", \"[\", \"]\"]:\n",
        "      pa = pa + 1\n",
        "    if j == \"?\":\n",
        "      q = q + 1 \n",
        "    if len(j) > 6:\n",
        "       l = l + 1\n",
        "    if j == 'I' or j == 'me' or j == 'i' or j == 'Me' or j =='My' or j == 'my' or j =='mine' or j =='Mine' or j == 'our' or j == 'Our' or j =='We' or j =='we' or j =='us' or j == 'Us':\n",
        "      ps = ps+1\n",
        "    if j == 'our' or j == 'Our' or j =='We' or j =='we' or j =='us' or j == 'Us':\n",
        "      pp = pp+1\n",
        "    if j == 'you' or j == 'You' or j =='your' or j =='Yours':\n",
        "      p2 = p2+1\n",
        "  im.append(p)\n",
        "  np.append(n)\n",
        "  nb.append(d)\n",
        "  pa.append(pa)\n",
        "  qm.append(q)\n",
        "  sl.append(l)\n",
        "  sr.append(ps)\n",
        "  we.append(pp)\n",
        "  yu.append(p2)"
      ],
      "metadata": {
        "id": "LWBEI-miQbXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pp = []\n",
        "pr = []\n",
        "for i in tokens:\n",
        "  count_prep = 0\n",
        "  count_pro = 0\n",
        "  for x,y in nltk.pos_tag(i):\n",
        "      if y == \"IN\": count_prep += 1\n",
        "      if y == \"PRP\" or y == \"PRP$\": count_pro += 1\n",
        "  pp.append(count_prep)\n",
        "  pr.append(count_pro)"
      ],
      "metadata": {
        "id": "zIzIIElDVmSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7af0f4-318d-42f9-e728-a4ac61fc44c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install better_profanity\n",
        "from better_profanity import profanity\n",
        "profanity.load_censor_words()\n",
        "\n",
        "text = \"You p1ec3 of sHit.\"\n",
        "censored_text = profanity.censor(text)\n",
        "print(censored_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2tgJKLnt-bq",
        "outputId": "52e70415-d9a9-4d70-9329-35808b365902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting better_profanity\n",
            "  Downloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████                         | 10 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 20 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 30 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 40 kB 28.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 46 kB 3.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: better-profanity\n",
            "Successfully installed better-profanity-0.7.0\n",
            "You p1ec3 of ****.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sw = []\n",
        "for i in tr[\"text\"]:\n",
        "    censored_text = profanity.censor(i)\n",
        "    a = set(i.split(\" \"))\n",
        "    b = set(censored_text.split(\" \"))\n",
        "    sw.append(len(b.difference(a)))"
      ],
      "metadata": {
        "id": "52BztDUXvrs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tt = []\n",
        "wc = []\n",
        "for i in tokens:\n",
        "  w = len(i)\n",
        "  T = len(set(i))\n",
        "  tt.append((w-T)/T)\n",
        "  wc.append(len(set(tokens))"
      ],
      "metadata": {
        "id": "4MPVcWb1zFh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Counter(text.split()).most_common()\n",
        "for j in range(len(a)):\n",
        "  print(a[j][1])"
      ],
      "metadata": {
        "id": "x_kQSZMIzQ1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "mf = []\n",
        "for i in tokens:\n",
        "  a = Counter(str(i).split()).most_common()\n",
        "  wf = 0\n",
        "  for j in range(len(a)):\n",
        "    wf = wf + a[j][1]\n",
        "  mf.append(wf/len(i))"
      ],
      "metadata": {
        "id": "lHoYYtrG8TJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = []\n",
        "s = []\n",
        "a = []\n",
        "c = []\n",
        "o = []"
      ],
      "metadata": {
        "id": "9KaSZhLz-SQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = -0.08*ap + -0.02*cm + -.07*du + -.05*el + 0*em + -.04*0 + 0.05*im + -0.08*np + -0.03*ne + -0.03*nb + -.06*pa + 0.07*pe + 0*pp + .07*pr + -0.06*qm + 0.07*sr + -0.06*sl + -0.01*sw + -0.05*tt + -0.01*wc + 0.06*we + -0.01*yu + 0.05*mf "
      ],
      "metadata": {
        "id": "sN-E3ePh7WD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tBOgrfie9HUM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}