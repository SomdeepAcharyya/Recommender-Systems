{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomdeepAcharyya/Recommender-Systems/blob/main/Cross_domain_recommendation_with_user_personality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUvrwIieeW11"
      },
      "outputs": [],
      "source": [
        "# Cross Domain Recommendation with User Personality\n",
        "# Hanfei Wanga, Yuan Zuo, Hong Li, Junjie Wub,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKwwKStkehy4"
      },
      "outputs": [],
      "source": [
        "# Recommendation using persoanlity of users by applying PMF on user and item latent features, \n",
        "# persoanlity obatined using 5 binary SVM classifiers on text based reviews and embedding of reviews done by PTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-PsCMD_ZofK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy, cmake\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtynI0WXZ6rf"
      },
      "outputs": [],
      "source": [
        "# Tripadvisor review Dataset\n",
        "\n",
        "path_of_file = r'/content/drive/MyDrive/Per_CD_RS/tripadvisor_reviews_with_country.csv'\n",
        "with open(path_of_file, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  tr = pd.read_csv(infile)\n",
        "tr = tr.rename(columns={\"username\":\"userId\", \"taObject\":\"itemId\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAqTBJIfFLZO"
      },
      "outputs": [],
      "source": [
        "# amazon magazine review Dataset\n",
        "\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/Amazon_Text_Video_Games.json'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  az = pd.read_json(infile, lines=True, nrows=12000)\n",
        "az = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# amazon review dataset magazines csv\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/video_games_pers_num_300.csv'\n",
        "\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  arr = pd.read_csv(infile)"
      ],
      "metadata": {
        "id": "QHUCgOXxqnAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr.shape\n",
        "arr = arr[['0','1','2','3','4']]\n",
        "arr.columns=['open', 'cons','extra','agree', 'neuro']"
      ],
      "metadata": {
        "id": "xK3P9BKjDr5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob0hmZjqdnRU"
      },
      "source": [
        "# Preprocessing of text of amazon dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4Ensi1nbwAW",
        "outputId": "f93d4d86-21db-44da-8b5d-189b73dcd1aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tweet-preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdeVs1ElcVuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2468bc-392e-4d53-db6c-dec0850ba97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import preprocessor as p\n",
        "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY,p.OPT.MENTION,p.OPT.HASHTAG, p.OPT.ESCAPE_CHAR, p.OPT.RESERVED)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from textblob import Word\n",
        "import re\n",
        "punctuation = re.compile(r'[-.?&!,:;()|0-9]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6EZNMqpdyT4"
      },
      "outputs": [],
      "source": [
        "ad['processed_text'] = \"\"\n",
        "ad['reviewText'].fillna(\" \")\n",
        "array_text = []\n",
        "for i in range(len(ad)):\n",
        "  x = ad['reviewText'][i]\n",
        "  word_tokens = word_tokenize(x) if type(x) != float else  \" \"\n",
        "  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  lemma_words = []\n",
        "  for words in filtered_sentence:\n",
        "      word = Word(words).lemmatize()\n",
        "      lemma_words.append(word)\n",
        "  punc_words = []\n",
        "  for words in lemma_words:\n",
        "      word =  punctuation.sub(\"\", words)\n",
        "      if len(word) > 0:\n",
        "        punc_words.append(word.lower())\n",
        "  line = \"\"\n",
        "  for i in punc_words:\n",
        "    line = line + \" \" + i.lower()\n",
        "  array_text.append(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p8H0pWceCtE"
      },
      "outputs": [],
      "source": [
        "ad['processed_text'] = array_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGlXTt7XYueB"
      },
      "outputs": [],
      "source": [
        "ad['processed_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLSIbDYKuuK8"
      },
      "source": [
        "# label generation and feature processing TripAdvisor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KGzzltUu6b6"
      },
      "outputs": [],
      "source": [
        "# tripadvisor dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8MYLSykvf8E"
      },
      "outputs": [],
      "source": [
        "openness = []\n",
        "for i in range(len(tr)):\n",
        "  if tr['open'][i] >= tr.open.mean():\n",
        "    openness.append(1)\n",
        "  else:\n",
        "    openness.append(0)\n",
        "tr['open_label'] = openness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7iSeWNbwP8-"
      },
      "outputs": [],
      "source": [
        "array = []\n",
        "for i in range(len(tr)):\n",
        "  if tr['cons'][i] >= tr.cons.mean():\n",
        "    array.append(1)\n",
        "  else:\n",
        "    array.append(0)\n",
        "tr['cons_label'] = array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IxV0_31w50x"
      },
      "outputs": [],
      "source": [
        "array = []\n",
        "for i in range(len(tr)):\n",
        "  if tr['extra'][i] >= tr.extra.mean():\n",
        "    array.append(1)\n",
        "  else:\n",
        "    array.append(0)\n",
        "tr['extra_label'] = array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp1xJrPbxEVU"
      },
      "outputs": [],
      "source": [
        "array = []\n",
        "for i in range(len(tr)):\n",
        "  if tr['agree'][i] >= tr.agree.mean():\n",
        "    array.append(1)\n",
        "  else:\n",
        "    array.append(0)\n",
        "tr['agree_label'] = array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yuwzB3BxJu9"
      },
      "outputs": [],
      "source": [
        "array = []\n",
        "for i in range(len(tr)):\n",
        "  if tr['neuro'][i] >= tr.neuro.mean():\n",
        "    array.append(1)\n",
        "  else:\n",
        "    array.append(0)\n",
        "tr['neuro_label'] = array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYDPRhKvmgoF"
      },
      "source": [
        "# Predictive Text embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cads3vmiEjax",
        "outputId": "29f6521b-1a77-45e0-f0c5-853ffb7cc525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting libsvm\n",
            "  Downloading libsvm-3.23.0.4.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 92 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 170 kB 5.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: libsvm\n",
            "  Building wheel for libsvm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libsvm: filename=libsvm-3.23.0.4-cp37-cp37m-linux_x86_64.whl size=233365 sha256=a7592e55fb0bac9f95dab880f14f3e566255270f55184ccf51483dbcd626b908\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/e8/1e/bf95cf256e4d3ffc94289ab508c49d48e34c98220af63e3513\n",
            "Successfully built libsvm\n",
            "Installing collected packages: libsvm\n",
            "Successfully installed libsvm-3.23.0.4\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import logging\n",
        "from theano import tensor as T\n",
        "import theano\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "!pip install libsvm\n",
        "from libsvm.svmutil import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpEstdtYMxTr"
      },
      "outputs": [],
      "source": [
        "X = tr['processed_text']\n",
        "y = tr['neuro_label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "tgt_text = ad['processed_text']\n",
        "overall = tr['processed_text']\n",
        "overall_l = tr['neuro_label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJX-j0xRAAX6"
      },
      "outputs": [],
      "source": [
        "#tr = df.reset_index()\n",
        "global json_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g0YBSZz_tSu"
      },
      "outputs": [],
      "source": [
        "# Class READ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FnlJd_6CJz5"
      },
      "outputs": [],
      "source": [
        "class READ():\n",
        "\tdef __init__(self,window_size):\n",
        "\t\tself.w={}  \t# word and their vertes id list\n",
        "\t\tself.wcount={} #list\n",
        "\t\tself.listoffreq=[]\n",
        "\t\tself.d=[]\t# document and their vertex id\n",
        "\t\tself.l={}\t# class label and their vertex id list\n",
        "\t\tself.w2w={}\t#word to word mapping with edge weight list\n",
        "\t\tself.w2d={}\t#word to document mapping with edge weight list\n",
        "\t\tself.w2l={}\t#word to class mapping with edge weight list\n",
        "\t\tself.ndocs=0\t#no of document \n",
        "\t\tself.nlabel=0\t#no of labels\n",
        "\t\tself.window_size=window_size;\n",
        "\t\tself.nedge=0\n",
        "\t\tself.nvertex=0\n",
        "\n",
        "\tdef generate_graphs(self):\n",
        "\t\tfiles = []\n",
        "\t\tlabels = []\n",
        "\t\tfor filename in range(len(overall)):\n",
        "\t\t\tfiles.append(overall[filename])\n",
        "\t\t\tif overall_l[filename] == 1:\n",
        "\t\t\t\tlabels.append('pos')\n",
        "\t\t\telif overall_l[filename] == 0:\n",
        "\t\t\t\tlabels.append('neg')\n",
        "        \n",
        "\t\tdocument=1\n",
        "\t\tnodes=1\n",
        "\t\tlabel=1\n",
        "\t\tfor x in labels:\n",
        "\t\t\tif x not in self.l:\n",
        "\t\t\t\tself.l[x]=label\n",
        "\t\t\t\tlabel+=1\n",
        "\t\tself.nlabel = label\n",
        "\n",
        "\t\tfor filename in files:\n",
        "\t\t\tfd=filename if type(filename)!= float else \" \"\n",
        "\t\t\tfor x in fd.split():\n",
        "\t\t\t\tx = x.strip()\n",
        "\t\t\t\tx=x.split()\n",
        "\t\t\t\t#if x[-1][-1:]=='\\n':\n",
        "\t\t\t\t#\tx[-1]=x[-1][:-1]\n",
        "\t\t\t\tself.d.append((x,labels[files.index(filename)],document))\n",
        "\t\t\t\tdocument+=1\n",
        "\t\t\t\tfor y in x:\n",
        "\t\t\t\t\tif y not in self.w:\n",
        "\t\t\t\t\t\tself.w[y]=nodes\n",
        "\t\t\t\t\t\tself.wcount[nodes]=1\n",
        "\t\t\t\t\t\tnodes+=1\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tself.wcount[self.w[y]]+=1\n",
        "\t\tself.ndocs = document\n",
        "\t\tself.nvertex = nodes-1\n",
        "\n",
        "\t\ti=1\n",
        "\t\t# print self.nvertex\n",
        "\t\twhile i <=self.nvertex:\n",
        "\t\t\tself.listoffreq.append(self.wcount[i])\n",
        "\t\t\ti= i+1\n",
        "\t\t\n",
        "\t\tfor x in self.d:\n",
        "\t\t\tindex=0\n",
        "\t\t\tlen1=len(x[0])\n",
        "\t\t\tlabel=self.l[x[1]]\n",
        "\t\t\tfor y in x[0]:\n",
        "\t\t\t\tword=self.w[y] #word is vertex id of \"word and document\" graph\n",
        "\t\t\t\tleft=index-self.window_size/2\n",
        "\t\t\t\tright=index+self.window_size/2\n",
        "\t\t\t\tif left<0:\n",
        "\t\t\t\t\tleft=0\n",
        "\t\t\t\tif right>len1:\n",
        "\t\t\t\t\tright=len1-1\n",
        "\n",
        "\t\t\t\t# word to word dictionary\n",
        "\t\t\t\tfor z in range(left,index):\n",
        "\t\t\t\t\ta=self.w[x[0][z]]\n",
        "\t\t\t\t\t# print a,word\n",
        "\t\t\t\t\tif a==word:\n",
        "\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\tif word not in self.w2w:\n",
        "\t\t\t\t\t\tself.w2w[word]={}\n",
        "\t\t\t\t\tb=self.w2w[word]\n",
        "\t\t\t\t\tif a not in b:\n",
        "\t\t\t\t\t\tb[a]=1\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tb[a]=b[a]+1\n",
        "\t\t\t\t\n",
        "\t\t\t\tfor z in range(index+1,right):\n",
        "\t\t\t\t\t# print x[0][z],y,z\n",
        "\t\t\t\t\ta=self.w[x[0][z]]\n",
        "\t\t\t\t\tif a==word:\n",
        "\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\tif word not in self.w2w:\n",
        "\t\t\t\t\t\tself.w2w[word]={}\n",
        "\t\t\t\t\tb=self.w2w[word]\n",
        "\t\t\t\t\tif a not in b:\n",
        "\t\t\t\t\t\tb[a]=1\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tb[a]=b[a]+1\n",
        "\n",
        "\t\t\t\t# document to word  dictionary\n",
        "\t\t\t\tif x[2] not in self.w2d:\n",
        "\t\t\t\t\tself.w2d[x[2]]={}\n",
        "\t\t\t\tif word not in self.w2d[x[2]]:\n",
        "\t\t\t\t\tself.w2d[x[2]][word]=0\n",
        "\t\t\t\tself.w2d[x[2]][word]=self.w2d[x[2]][word]+1\t\t\t\t\n",
        "\t\t\t\t\n",
        "\t\t\t\t# word to label dictionary\n",
        "\t\t\t\tif label not in self.w2l:\n",
        "\t\t\t\t\tself.w2l[label]={}\n",
        "\t\t\t\tif word not in self.w2l[label]:\n",
        "\t\t\t\t\tself.w2l[label][word]=0\n",
        "\t\t\t\tself.w2l[label][word] = self.w2l[label][word]+1\n",
        "\t\t\t\tindex+=1\n",
        "\n",
        "\t\tself.json_array = self.w #json.dump(np.array(self.w), open('word_mapping.json', 'wb'))\n",
        "  # json.dump(self.l, open('label_mapping.json', 'wb'))\n",
        "  #       json.dump(self.d, open('document_mapping.json', 'wb'))\n",
        "  #       print 'w2l', len(self.w2l.keys())\n",
        "  #       print 'w2d', len(self.w2d.keys())\n",
        "  #       print 'w2w', len(self.w2w.keys())\n",
        "\n",
        "\tdef gen_edgeprob(self):\n",
        "\t\tp = []\t#probability of edge b/w w1-w2\n",
        "\t\tv1 = []\t#w1\n",
        "\t\tv2 = [] #w2\n",
        "\t\tpd = [] #probability of edge b/w w-d\n",
        "\t\tv3 = [] #d\n",
        "\t\tv4 = []\t#w\n",
        "\t\tpl = [] #probability of edge b/w w-l\n",
        "\t\tv5 = [] #l\n",
        "\t\tv6 = [] #w\n",
        "\t\tfor k in self.w2w.keys():\n",
        "\t\t\tfor kj in self.w2w[k].keys():\n",
        "\t\t\t\tp.append(self.w2w[k][kj])#all co-occurences added\n",
        "\t\t\t\tv1.append(k)\n",
        "\t\t\t\tv2.append(kj)\n",
        "\t\t\t\tself.nedge += 1\n",
        "\t\tp = np.asarray(p, dtype=np.float64)\n",
        "\t\tnP=np.asarray(self.listoffreq, dtype=np.float64)\n",
        "\t\tnP = np.power(nP, (3.0/4.0))\n",
        "\t\tp = p / float(sum(p))\n",
        "\t\tnP = nP / float(sum(nP))\n",
        "\n",
        "\t\tfor doc in self.w2d.keys():\n",
        "\t\t\tfor word in self.w2d[doc].keys():\n",
        "\t\t\t\tpd.append(self.w2d[doc][word])#frequency of that word in the doc\n",
        "\t\t\t\tv3.append(doc)\n",
        "\t\t\t\tv4.append(word)\n",
        "\t\tpd = np.asarray(pd, dtype=np.float64)\n",
        "\t\tpd = pd / float(sum(pd))\n",
        "\n",
        "\t\tfor label in self.w2l.keys():\n",
        "\t\t\tfor word in self.w2l[label].keys():\n",
        "\t\t\t\tpl.append(self.w2l[label][word])#frequency of that word in that label\n",
        "\t\t\t\tv5.append(label)\n",
        "\t\t\t\tv6.append(word)\n",
        "\t\tpl = np.asarray(pl, dtype=np.float64)\n",
        "\t\tpl = pl / float(sum(pl))\n",
        "\t\t\n",
        "\t\treturn p, nP, v1, v2, pd, v3, v4, pl, v5, v6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaH331GiDCK-"
      },
      "outputs": [],
      "source": [
        "# class PTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za5A2ETQED2N"
      },
      "outputs": [],
      "source": [
        "from theano import tensor as T\n",
        "import theano\n",
        "import numpy as np\n",
        "\n",
        "class PTE(object):\n",
        "    '''\n",
        "    Defines the PTE model (cost function, parameters in theano.\n",
        "    '''\n",
        "    def __init__(self, nvertex, out_dim, ndocs, nlabels, lr=0.05):\n",
        "        '''\n",
        "        Parameters specs:\n",
        "            nvertex : no of vertices in the graph\n",
        "            out_dim : node vector dimension\n",
        "            ndocs : number of documents in the corpus\n",
        "            nlabels : number of labels\n",
        "            lr : learning rate.\n",
        "        '''\n",
        "        # TO-DO: Try initialization from uniform\n",
        "        # Initialize model paramters\n",
        "        eps = np.sqrt(1.0 / float(out_dim))\n",
        "        self.w = np.asarray(np.random.uniform(low=-eps, high=eps, size=(nvertex, out_dim)),\n",
        "                       dtype=theano.config.floatX)\n",
        "        self.d = np.asarray(np.random.uniform(low=-eps, high=eps, size=(ndocs, out_dim)),\n",
        "                       dtype=theano.config.floatX)\n",
        "        self.l = np.asarray(np.random.uniform(low=-eps, high=eps, size=(nlabels, out_dim)),\n",
        "                       dtype=theano.config.floatX)\n",
        "        self.W = theano.shared(self.w, name='W', borrow=True)\n",
        "        self.D = theano.shared(self.d, name='D', borrow=True)\n",
        "        self.L = theano.shared(self.l, name='L', borrow=True)\n",
        "        self.lr = lr\n",
        "\n",
        "    def ww_model(self):\n",
        "        '''\n",
        "        Performs SGD update (pre-training on ww graph).\n",
        "        '''\n",
        "        indm = T.iscalar()\n",
        "        indc = T.iscalar()\n",
        "        indr = T.ivector()\t\t#vector of 5 negative edge samples\n",
        "        Uj = self.W[indm, :] #one row of W\n",
        "        Ui = self.W[indc, :] #one row of W\n",
        "        Ui_Set = self.W[indr, :] #rows for negative edge sampling\n",
        "        cost_ww = T.log(T.nnet.sigmoid(T.dot(Ui, Uj)))\n",
        "        cost_ww -= T.log(T.sum(T.nnet.sigmoid(T.sum(Uj * Ui_Set, axis=1))))\n",
        "        #cost_ww = T.dot(Ui, Uj)\n",
        "        #cost_ww -= T.log(T.sum(T.exp(T.sum(Uj * Ui_Set, axis=1))))\n",
        "        cost = -cost_ww\n",
        "        grad_ww = T.grad(cost, [Uj, Ui, Ui_Set]) #gradient w.r.t 3 variables\n",
        "        deltaW = T.inc_subtensor(self.W[indm, :], - (self.lr) * grad_ww[0])\n",
        "        deltaW = T.inc_subtensor(deltaW[indc, :], - (self.lr) * grad_ww[1])\n",
        "        deltaW = T.inc_subtensor(deltaW[indr, :], - (self.lr) * grad_ww[2])\n",
        "        updateD = [(self.W, deltaW)]\n",
        "        self.train_ww = theano.function(inputs=[indm, indc, indr], outputs=cost, updates=updateD)\n",
        "\n",
        "    def pretraining_ww(self, indm, indc, indr):\n",
        "        return self.train_ww(indm, indc, indr)\n",
        "\n",
        "    def wd_model(self):\n",
        "        '''\n",
        "        Performs SGD update (pre-training on wd graph).\n",
        "        '''\n",
        "        indm = T.iscalar()\n",
        "        indc = T.iscalar()\n",
        "        indr = T.ivector()\t\t#vector of 5 negative edge samples\n",
        "        Uj = self.D[indm, :] #one row of D\n",
        "        Ui = self.W[indc, :] #one row of W\n",
        "        Ui_Set = self.W[indr, :] #rows of W for negative edge sampling\n",
        "        cost_wd = T.log(T.nnet.sigmoid(T.dot(Ui, Uj)))\n",
        "        cost_wd -= T.log(T.sum(T.nnet.sigmoid(T.sum(Uj * Ui_Set, axis=1))))\n",
        "        cost = -cost_wd\n",
        "        grad_wd = T.grad(cost, [Uj, Ui, Ui_Set]) #gradient w.r.t 3 variables\n",
        "        \n",
        "        deltaD = T.inc_subtensor(self.D[indm, :], - (self.lr) * grad_wd[0])\n",
        "        deltaW = T.inc_subtensor(self.W[indc, :], - (self.lr) * grad_wd[1])\n",
        "        deltaW = T.inc_subtensor(deltaW[indr, :], - (self.lr) * grad_wd[2])\n",
        "        updateD = [(self.W, deltaW), (self.D, deltaD)]\n",
        "        self.train_wd = theano.function(inputs=[indm, indc, indr], outputs=cost, updates=updateD)\n",
        "\n",
        "    def pretraining_wd(self, indm, indc, indr):\n",
        "        return self.train_wd(indm, indc, indr)\n",
        "\n",
        "    def wl_model(self):\n",
        "        '''\n",
        "        Performs SGD update (pre-training on wd graph).\n",
        "        '''\n",
        "        indm = T.iscalar()\n",
        "        indc = T.iscalar()\n",
        "        indr = T.ivector()\t\t#vector of 5 negative edge samples\n",
        "        Uj = self.L[indm, :] #one row of L\n",
        "        Ui = self.W[indc, :] #one row of W\n",
        "        Ui_Set = self.W[indr, :] #rows of W for negative edge sampling\n",
        "        cost_wl = T.log(T.nnet.sigmoid(T.dot(Ui, Uj)))\n",
        "        cost_wl -= T.log(T.sum(T.nnet.sigmoid(T.sum(Uj * Ui_Set, axis=1))))\n",
        "        cost = -cost_wl\n",
        "        grad_wl = T.grad(cost, [Uj, Ui, Ui_Set]) #gradient w.r.t 3 variables\n",
        "        \n",
        "        deltaL = T.inc_subtensor(self.L[indm, :], - (self.lr) * grad_wl[0])\n",
        "        deltaW = T.inc_subtensor(self.W[indc, :], - (self.lr) * grad_wl[1])\n",
        "        deltaW = T.inc_subtensor(deltaW[indr, :], - (self.lr) * grad_wl[2])\n",
        "        updateD = [(self.W, deltaW), (self.L, deltaL)]\n",
        "        self.train_wl = theano.function(inputs=[indm, indc, indr], outputs=cost, updates=updateD)\n",
        "\n",
        "    def pretraining_wl(self, indm, indc, indr):\n",
        "        return self.train_wl(indm, indc, indr)\n",
        "\n",
        "    def save_model(self):\n",
        "        '''\n",
        "        Save embedding matrices on disk\n",
        "        '''\n",
        "        W = self.W.get_value()\n",
        "        np.save('lookupW', W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m8dqk4OEHgJ"
      },
      "outputs": [],
      "source": [
        "# class train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFIXWCzkEatt",
        "outputId": "1d5afaa2-5741-4dc2-d75d-554767b87a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read and graphing complete\n",
            "saving the model finally\n"
          ]
        }
      ],
      "source": [
        "class train_pte(object):\n",
        "\n",
        "\t#Initialize graph\n",
        "\tdef __init__(self):\n",
        "\t\tself.window_size=300\n",
        "\t\tself.graphs = READ(self.window_size) #Give window size as parameter\n",
        "\t\tself.graphs.generate_graphs()\n",
        "\t\tprint(\"Read and graphing complete\")\n",
        "\t\tself.ndims = 300\n",
        "\t\tself.lr = 0.005\n",
        "\t\tself.batch_size = 100\n",
        "\t\tself.k = 6\n",
        "\t\tself.nepochs = 1\n",
        "\n",
        "\tdef train(self):\n",
        "\t\tp, nP, v1, v2, pd, v3, v4, pl, v5, v6  = self.graphs.gen_edgeprob()\n",
        "\t\tpte = PTE(self.graphs.nvertex, self.ndims, self.graphs.ndocs, self.graphs.nlabel)\n",
        "\t\tpte.ww_model()\n",
        "\t\tpte.wd_model()\n",
        "\t\tpte.wl_model()\n",
        "\t\tcurrentHour = datetime.utcnow().hour\n",
        "\n",
        "\t\t# setting up logger\n",
        "\t\tlogger = logging.getLogger(\"wordTovec\")\n",
        "\t\tlogger.setLevel(logging.INFO)\n",
        "\t\tlogger.setLevel(logging.DEBUG)\n",
        "\t\tfh = logging.FileHandler(\"word2graph2vec.log\")\n",
        "\t\tformatter = logging.Formatter('%(asctime)s %(message)s')\n",
        "\t\tfh.setFormatter(formatter)\n",
        "\t\tlogger.addHandler(fh)\n",
        "\t\tlogger.info(\"Training started\")\n",
        "\t\tlogger.info(\"Total edges : %f \" % self.graphs.nedge)\n",
        "\n",
        "\t\tfor it in range(0, self.graphs.nedge, self.batch_size):\n",
        "\t\t\tsample = np.random.choice(p.shape[0], self.batch_size, p=p)\n",
        "\t\t\tk=0\n",
        "\t\t\twhile k < sample.shape[0]:\n",
        "\t\t\t\ti = v1[sample[k]]-1\n",
        "\t\t\t\tj = v2[sample[k]]-1\n",
        "\t\t\t\ti_set = np.asarray(np.random.choice(self.graphs.nvertex, size=self.k, p=nP), dtype=np.int32)\n",
        "\t\t\t\tif i in i_set:\n",
        "\t\t\t\t\ti_set = np.delete(i_set, np.where(i_set==i))\n",
        "\t\t\t\tcostww = pte.pretraining_ww(j, i, i_set)\n",
        "\t\t\t\tk = k + 1\n",
        "\t\t\t\n",
        "\t\t\tsample = np.random.choice(pd.shape[0], self.batch_size, p=pd)\n",
        "\t\t\tk=0\n",
        "\t\t\twhile k < sample.shape[0]:\n",
        "\t\t\t\ti = v4[sample[k]]-1\n",
        "\t\t\t\tj = v3[sample[k]]-1\n",
        "\t\t\t\ti_set = np.asarray(np.random.choice(self.graphs.nvertex, size=self.k, p=nP), dtype=np.int32)\n",
        "\t\t\t\tif i in i_set:\n",
        "\t\t\t\t\ti_set = np.delete(i_set, np.where(i_set==i))\n",
        "\t\t\t\tcostwd = pte.pretraining_wd(j, i, i_set)\n",
        "\t\t\t\tk = k+1\n",
        "\n",
        "\t\t\tsample = np.random.choice(pl.shape[0], self.batch_size, p=pl)\n",
        "\t\t\tk=0\n",
        "\t\t\twhile k < sample.shape[0]:\n",
        "\t\t\t\ti = v6[sample[k]]-1\t#one word\n",
        "\t\t\t\tj = v5[sample[k]]-1\t#one label\n",
        "\t\t\t\ti_set = np.asarray(np.random.choice(self.graphs.nvertex, size=self.k, p=nP), dtype=np.int32)\n",
        "\t\t\t\tif i in i_set:\n",
        "\t\t\t\t\ti_set = np.delete(i_set, np.where(i_set==i))\n",
        "\t\t\t\tcostwl = pte.pretraining_wl(j, i, i_set)\n",
        "\t\t\t\tk = k+1\n",
        "\n",
        "\t\t\t#print(\"Current it: \", it, \" complete of total: \", self.graphs.nedge)\n",
        "\t\t\tif datetime.utcnow().hour >= currentHour+2:\n",
        "\t\t\t\tlogger.info(\"ww Cost after 2 hrs training is %f\" % costww)\n",
        "\t\t\t\tlogger.info(\"wd Cost after 2 hrs training is %f\" % costwd)\n",
        "\t\t\t\tlogger.info(\"wl Cost after 2 hrs training is %f\" % costwl)\n",
        "\t\t\t\tlogger.info(\"Current it: %f \" % it)\n",
        "\t\t\t\tlogger.info(\"Saving the model\")\n",
        "\t\t\t\tpte.save_model()\n",
        "\t\t\t\tcurrentHour += 2\n",
        "\t\tlogger.info(\"Saving the model finally\")\n",
        "\t\tprint(\"saving the model finally\")\n",
        "\t\tpte.save_model()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tpte = train_pte()\n",
        "\tpte.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXrT0wVnqvy5"
      },
      "outputs": [],
      "source": [
        "# test class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsz3JmP7s3rx",
        "outputId": "66da3a11-f165-4296-b69a-a338392b2286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 score_train:  0.8086867709509218\n",
            "acc score_train:  0.8369525380245556\n",
            "f1 score_test:  0.5128979781547757\n",
            "acc score_test:  0.6101190476190477\n"
          ]
        }
      ],
      "source": [
        "class test_pte(object):\n",
        "\n",
        "  def __init__(self, train_text, train_labels, test_text, test_labels, tgt_text):\n",
        "    self.all_words = pte.graphs.w\n",
        "    self.train_documents = []\n",
        "    self.train_text = train_text\n",
        "    self.train_labels = train_labels\n",
        "    self.test_text = test_text\n",
        "    self.test_documents = []\n",
        "    self.test_labels = test_labels\n",
        "    self.test_predicted_labels = []\n",
        "    self.tgt_text = tgt_text\n",
        "    self.tgt_predicted_labels = []\n",
        "    self.tgt_documents = []\n",
        "    self.tr_f1 = 0.0\n",
        "    self.tr_acc = 0.0\n",
        "    self.ts_f1 = 0.0\n",
        "    self.ts_acc = 0.0\n",
        "\n",
        "    self.word_embedding = np.load('lookupW.npy')\n",
        "    self.D = 300\n",
        "\n",
        "  def load_train_data(self):\n",
        "    train_set = []\n",
        "    document_no = 1\n",
        "    files = np.array(self.train_text)\n",
        "    class_labels = ['pos','neg']\n",
        "    index = 0\n",
        "    for file_name in files:\n",
        "      #fp = open(filename)\n",
        "      lines = file_name\n",
        "      words = lines.split(\" \") if type(lines) != float else \" \"\n",
        "      document = (words,class_labels[index],document_no)\n",
        "      train_set.append(document)\n",
        "      document_no += 1\n",
        "    index+=1\n",
        "    return train_set\n",
        "\n",
        "  def load_test_data(self):\n",
        "    test_set = []\n",
        "    document_no = 1\n",
        "    files = np.array(self.test_text)\n",
        "    class_labels = ['pos','neg']\n",
        "    index = 0\n",
        "    for file_name in files:\n",
        "      #fp = open(filename)\n",
        "      lines = str(file_name)\n",
        "      words = lines.split(\" \") if type(lines) != float else \" \"\n",
        "      document = (words,class_labels[index],document_no)\n",
        "      test_set.append(document)\n",
        "      document_no += 1\n",
        "    index+=1\n",
        "    return test_set\n",
        "\n",
        "  def load_tgt_data(self):\n",
        "    tgt_set = []\n",
        "    document_no = 1\n",
        "    files = np.array(self.tgt_text)\n",
        "    class_labels = ['pos','neg']\n",
        "    index = 0\n",
        "    for file_name in files:\n",
        "      #fp = open(filename)\n",
        "      lines = str(file_name)\n",
        "      words = lines.split(\" \") if type(lines) != float else \" \"\n",
        "      document = (words,class_labels[index],document_no)\n",
        "      tgt_set.append(document)\n",
        "      document_no += 1\n",
        "    index+=1\n",
        "    return tgt_set\n",
        "  \n",
        "  def test(self):\n",
        "    train_set = self.load_train_data()\n",
        "    test_set = self.load_test_data()\n",
        "    tgt_set = self.load_tgt_data()\n",
        "\n",
        "    for index in range(len(train_set)):\n",
        "      document_sum = np.zeros(self.D)\n",
        "      for word_index in range(len(train_set[index][0])-1):\n",
        "        word = train_set[index][0][word_index+1]\n",
        "        i = self.all_words[word]\n",
        "        embedding = self.word_embedding[i-1]\n",
        "        document_sum = np.add(document_sum, embedding)\n",
        "      document_average = np.divide(document_sum, len(train_set[index][0]))\n",
        "      self.train_documents.append(document_average)\n",
        "      #self.train_labels.append(train_set[index][1])\n",
        "      self.train_labels = np.array(self.train_labels)\n",
        "\n",
        "    for index in range(len(test_set)):\n",
        "      document_sum = np.zeros(self.D)\n",
        "      for word_index in range(len(test_set[index][0])-1):\n",
        "          word = test_set[index][0][word_index+1]\n",
        "          try:\n",
        "            i = self.all_words[word]\n",
        "          except:\n",
        "            i = 0\n",
        "          embedding = self.word_embedding[i-1]\n",
        "          document_sum = np.add(document_sum, embedding)\n",
        "      document_average = np.divide(document_sum, len(test_set[index][0]))\n",
        "      self.test_documents.append(document_average)\n",
        "      #self.test_labels.append(test_set[index][1])\n",
        "      self.test_actual_labels = np.array(self.test_labels)    \n",
        "\n",
        "    for index in range(len(tgt_set)):\n",
        "      document_sum = np.zeros(self.D)\n",
        "      for word_index in range(len(tgt_set[index][0])-1):\n",
        "          word = tgt_set[index][0][word_index+1]\n",
        "          try:\n",
        "            i = self.all_words[word]\n",
        "          except:\n",
        "            i = 0\n",
        "          embedding = self.word_embedding[i-1]\n",
        "          document_sum = np.add(document_sum, embedding)\n",
        "      document_average = np.divide(document_sum, len(tgt_set[index][0]))\n",
        "      self.tgt_documents.append(document_average)\n",
        "\n",
        "    clf = svm.SVC()\n",
        "    clf.fit(self.train_documents, self.train_labels)\n",
        "    \n",
        "    \n",
        "    from sklearn.metrics import f1_score, accuracy_score\n",
        "    self.train_predicted_labels = clf.predict(self.train_documents)\n",
        "    self.tr_f1 = f1_score(self.train_labels, self.train_predicted_labels)\n",
        "    print(\"f1 score_train: \", self.tr_f1)\n",
        "    self.tr_acc = accuracy_score(self.train_labels, self.train_predicted_labels)\n",
        "    print(\"acc score_train: \", self.tr_acc)\n",
        "    self.test_predicted_labels = clf.predict(self.test_documents)\n",
        "    self.ts_f1 = f1_score(self.test_actual_labels, self.test_predicted_labels)\n",
        "    print(\"f1 score_test: \", self.ts_f1)\n",
        "    self.ts_acc = accuracy_score(self.test_actual_labels, self.test_predicted_labels)\n",
        "    print(\"acc score_test: \", self.ts_acc)\n",
        "\n",
        "    self.tgt_predicted_labels = clf.predict(self.tgt_documents)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  pte_test = test_pte(X_train, y_train, X_test, y_test, tgt_text)\n",
        "  pte_test.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw0RoeUFLO1r",
        "outputId": "b9ad32b6-22a5-4312-b20b-7e5875380eb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "tgt_pers_neuro = pte_test.tgt_predicted_labels\n",
        "tgt_pers_neuro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6N0VG1W2GCk",
        "outputId": "aa73f3cc-c9ae-438f-d4ea-fae3f3df922f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11002"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "list(tgt_pers_open).count(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toygyrZ5Dv0L"
      },
      "outputs": [],
      "source": [
        "column_names = [\"open\", \"cons\", \"extra\", 'agree', 'neuro']\n",
        "df = pd.DataFrame(columns = column_names)\n",
        "df.open = tgt_pers_open\n",
        "df.cons = tgt_pers_cons\n",
        "df.extra = tgt_pers_extra\n",
        "df.agree = tgt_pers_agree\n",
        "df.neuro = tgt_pers_neuro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDrI_9XzExC2"
      },
      "outputs": [],
      "source": [
        "df.to_csv(r'/content/drive/MyDrive/Per_CD_RS/amazon_movies_pers_pte_binary.csv', encoding = 'utf-8-sig') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBlJab83wB45"
      },
      "source": [
        "# Probabilistic matrix factorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzg3UJ3u5Dtw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#arr = arr[0:12000]\n",
        "df = pd.DataFrame(arr)"
      ],
      "metadata": {
        "id": "UtHTDxjMsXx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDi0TjLMvGr9"
      },
      "outputs": [],
      "source": [
        "df['userId'] = az['userId']\n",
        "df['itemId'] = az['itemId']\n",
        "df['rating'] = az['rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4WetHFlvWoP"
      },
      "outputs": [],
      "source": [
        "df.columns = ['open', 'cons', 'extra', 'agree', 'neuro', 'userId', 'itemId', 'rating'] "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "WpFOC-SlrLsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEemf-8IEP32",
        "outputId": "7e611ebe-b2a3-43a4-e1c1-f96e829c7694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10633, 170)\n"
          ]
        }
      ],
      "source": [
        "    user_to_row = {}\n",
        "    item_to_column = {}\n",
        "    n_dims = 10\n",
        "    n_features = 5\n",
        "    parameters = {}\n",
        "    train_size = 0.8\n",
        "\n",
        "    uniq_users = np.unique(df.userId)\n",
        "    uniq_items = np.unique(df.itemId)\n",
        "\n",
        "    for i, user_id in enumerate(uniq_users):\n",
        "        user_to_row[user_id] = i\n",
        "\n",
        "    for j, item_id in enumerate(uniq_items):\n",
        "        item_to_column[item_id] = j\n",
        "    \n",
        "    n_users = len(uniq_users)\n",
        "    n_items = len(uniq_items)\n",
        "    \n",
        "    R = np.zeros((n_users, n_items))\n",
        "    \n",
        "    df_copy = df.copy()\n",
        "    train_set = df_copy.sample(frac=train_size).reset_index()\n",
        "    user_features_train = np.array(train_set[['open', 'cons', 'extra', 'agree', 'neuro']].fillna(0))\n",
        "    test_set = df_copy.drop(train_set.index).reset_index()\n",
        "    user_features_test = np.array(test_set[['open', 'cons', 'extra', 'agree', 'neuro']].fillna(0))\n",
        "    user_features_unique = np.array(df.groupby(by='userId').mean()[['open', 'cons', 'extra', 'agree', 'neuro']].fillna(0))\n",
        "    \n",
        "    for index, row in train_set.iterrows():\n",
        "        i = user_to_row[row.userId]\n",
        "        j = item_to_column[row.itemId]\n",
        "        R[i, j] = row.rating\n",
        "    print(R.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpg5t1LX-4Nj"
      },
      "outputs": [],
      "source": [
        "rating =[]\n",
        "for i in range(n_users):\n",
        "  rating.append(np.average(R[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcS-SrR29H-3"
      },
      "outputs": [],
      "source": [
        "# DO NOT RUN THIS\n",
        "\n",
        "def get_ratings_matrix(df, train_size=0.75):\n",
        "    user_to_row = {}\n",
        "    item_to_column = {}\n",
        "    df_values = df.values\n",
        "    n_dims = 10\n",
        "    n_features = 5\n",
        "    parameters = {}\n",
        "    \n",
        "    uniq_users = np.unique(df_values[:, 3])\n",
        "    uniq_items = np.unique(df_values[:, 0])\n",
        "\n",
        "    for i, user_id in enumerate(uniq_users):\n",
        "        user_to_row[user_id] = i\n",
        "\n",
        "    for j, item_id in enumerate(uniq_items):\n",
        "        item_to_column[item_id] = j\n",
        "    \n",
        "    n_users = len(uniq_users)\n",
        "    n_items = len(uniq_items)\n",
        "    \n",
        "    R = np.zeros((n_users, n_items))\n",
        "    \n",
        "    df_copy = df.copy()\n",
        "    train_set = df_copy.sample(frac=train_size, random_state=0)\n",
        "    test_set = df_copy.drop(train_set.index)\n",
        "    \n",
        "    for index, row in train_set.iterrows():\n",
        "        i = user_to_row[row.userId]\n",
        "        j = item_to_column[row.itemId]\n",
        "        R[i, j] = row.rating\n",
        "\n",
        "    return R, train_set, test_set, n_dims, n_users, n_items, user_to_row, item_to_column, n_features\n",
        "\n",
        "R, train_set, test_set, n_dims, n_users, n_items, user_to_row, item_to_column, n_features = get_ratings_matrix(tr, 0.8)\n",
        "parameters = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "956Hg4zFwGVb"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(lambda_U, lambda_V, lambda_W, user_features):\n",
        "    U = np.random.normal(0.0, 1.0 / lambda_V, (n_dims, n_users))    #np.zeros((n_dims, n_users), dtype=np.float64)\n",
        "    V = np.random.normal(0.0, 1.0 / lambda_V, (n_dims, n_items))\n",
        "    print(len(rating), len(user_features))\n",
        "    op, _ = pearsonr(rating, user_features[:,0])\n",
        "    cn, _ = pearsonr(rating, user_features[:,1])\n",
        "    ex, _ = pearsonr(rating, user_features[:,2])\n",
        "    ag, _ = pearsonr(rating, user_features[:,3])\n",
        "    ne, _ = pearsonr(rating, user_features[:,4])\n",
        "    W = [op, cn, ex, ag, ne]     # for persoanlity boosted PMF 5 persoanlity traits n_features=5\n",
        "    \n",
        "    \n",
        "    parameters['U'] = U\n",
        "    parameters['V'] = V\n",
        "    parameters['W'] = W   ##\n",
        "    parameters['lambda_U'] = lambda_U\n",
        "    parameters['lambda_V'] = lambda_V\n",
        "    parameters['lambda_W'] = lambda_W  ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czsh0jxLwM0C"
      },
      "outputs": [],
      "source": [
        "def update_parameters():\n",
        "    U = parameters['U']\n",
        "    V = parameters['V']\n",
        "    W = parameters['W']  ##\n",
        "    lambda_U = parameters['lambda_U']\n",
        "    lambda_V = parameters['lambda_V']\n",
        "    lambda_W = parameters['lambda_W']   ##\n",
        "\n",
        "    for i in range(n_users):\n",
        "        V_j = V[:, R[i, :] > 0]\n",
        "        U[:, i] = np.dot(np.linalg.inv(np.dot(V_j, V_j.T) + lambda_U * np.identity(n_dims)), np.dot(R[i, R[i, :] > 0], V_j.T))\n",
        "      \n",
        "    for j in range(n_items):\n",
        "        U_i = U[:, R[:, j] > 0]\n",
        "        V[:, j] = np.dot(np.linalg.inv(np.dot(U_i, U_i.T) + lambda_V * np.identity(n_dims)), np.dot(R[R[:, j] > 0, j], U_i.T))\n",
        "           \n",
        "    parameters['U'] = U\n",
        "    parameters['V'] = V\n",
        "    parameters['W'] = W ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4MdxMZa5XwY"
      },
      "outputs": [],
      "source": [
        "def update_max_min_ratings(user_features):\n",
        "    U = parameters['U'].T\n",
        "    V = parameters['V']\n",
        "    W = parameters['W']  ##\n",
        "    P = user_features  ##\n",
        "\n",
        "##\n",
        "    for i in range(n_users):\n",
        "      t = 0\n",
        "      for k in range(n_features):\n",
        "        t += P[i][k] * W[k]\n",
        "      t = t / np.sum(P[i][k])\n",
        "      U[i] = np.add(U[i], t)\n",
        "\n",
        "    R = U @ V\n",
        "    R[R>5] = 5\n",
        "    R[R<0] = 0\n",
        "    R = np.nan_to_num(R)\n",
        "    min_rating = np.min(R)\n",
        "    max_rating = np.max(R)\n",
        "\n",
        "    parameters['min_rating'] = min_rating\n",
        "    parameters['max_rating'] = max_rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXs7vUw9wnur"
      },
      "outputs": [],
      "source": [
        "def log_a_posteriori(user_features):\n",
        "    lambda_U = parameters['lambda_U']\n",
        "    lambda_V = parameters['lambda_V']\n",
        "    lambda_W = parameters['lambda_W'] ##\n",
        "    U = parameters['U']\n",
        "    V = parameters['V']\n",
        "    W = parameters['W']  ##\n",
        "    P = user_features    ##\n",
        "\n",
        "    UV = np.dot(U.T, V)\n",
        "    R_UV = (R[R > 0] - UV[R > 0])\n",
        "    W = np.array(W)\n",
        "    \n",
        "    return -0.5 * (np.sum(np.dot(R_UV, R_UV.T)) + lambda_U * np.sum(np.dot(U, U.T)) + lambda_V * np.sum(np.dot(V, V.T)) + lambda_W * np.sum(np.dot(W, W.T)))  ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na09EUk05kYH"
      },
      "outputs": [],
      "source": [
        "def predict(user_id, item_id, user_features):\n",
        "    U = parameters['U']\n",
        "    V = parameters['V']\n",
        "    W = parameters['W']   ##\n",
        "    P = user_features     ##\n",
        "  \n",
        "    r_ij = U[:, user_to_row[user_id]].reshape(1, -1) @ V[:, item_to_column[item_id]].reshape(-1, 1)\n",
        "    r_ij[r_ij<0] = 0\n",
        "    r_ij[r_ij>5] = 5\n",
        "    r_ij = np.nan_to_num(r_ij)\n",
        "\n",
        "    max_rating = parameters['max_rating']\n",
        "    min_rating = parameters['min_rating']\n",
        "\n",
        "    return 0 if max_rating == min_rating else ((r_ij[0][0] - min_rating) / (max_rating - min_rating)) * 5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2i6plpRwt6q"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataset, user_features):\n",
        "    ground_truths = []\n",
        "    predictions = []\n",
        "    \n",
        "    for index, row in dataset.iterrows():\n",
        "        ground_truths.append(row.loc['rating'])\n",
        "        t = predict(row.loc['userId'], row.loc['itemId'], user_features)\n",
        "        predictions.append(t)   ##\n",
        "    \n",
        "    print(\"mae\", mean_absolute_error(ground_truths, predictions), \"rmse\", mean_squared_error(ground_truths, predictions))\n",
        "    return mean_squared_error(ground_truths, predictions, squared=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1TcCysewoiS"
      },
      "outputs": [],
      "source": [
        "def train(n_epochs, user_features_train, user_features_test):\n",
        "    user_features_total = np.array(pd.concat([pd.DataFrame(user_features_train), pd.DataFrame(user_features_test)], axis=0))  ##\n",
        "    initialize_parameters(0.02, 0.02, 0.01, user_features_unique)   ##\n",
        "    print(\"initialization done\")\n",
        "    log_aps = []\n",
        "    rmse_train = []\n",
        "    rmse_test = []\n",
        "\n",
        "\n",
        "    update_max_min_ratings(user_features_total)\n",
        "    print(\"update_max_min_ratings 1 done\")\n",
        "    rmse_train.append(evaluate(train_set, user_features_train))\n",
        "    rmse_test.append(evaluate(test_set, user_features_test))\n",
        "    print(\"rmse done\")\n",
        "    #print(\"train\", rmse_train)\n",
        "    #print(\"test\", rmse_test)\n",
        "    print(\"starting epochs\")\n",
        "    \n",
        "    for k in range(n_epochs):\n",
        "        update_parameters()\n",
        "        log_ap = log_a_posteriori(user_features_total)  ##\n",
        "        log_aps.append(log_ap)\n",
        "\n",
        "        if (k + 1) % 10 == 0:\n",
        "            rmse_train.append(evaluate(train_set, user_features_train))    ##\n",
        "            rmse_test.append(evaluate(test_set, user_features_test))       ##\n",
        "            print('Log p a-posteriori at iteration', k + 1, ':', log_ap)\n",
        "\n",
        "    update_max_min_ratings(user_features_total)\n",
        "\n",
        "    return log_aps, rmse_train, rmse_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5LVPoQz5vIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f5ee55-f8f6-405b-e079-babc79caf4ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10633 10633\n",
            "initialization done\n",
            "update_max_min_ratings 1 done\n",
            "mae 2.5261159098508505 rmse 11.136698817888746\n",
            "mae 2.568466626197096 rmse 11.415842397952646\n",
            "rmse done\n",
            "starting epochs\n",
            "mae 0.13999034860506201 rmse 0.025139143415427705\n",
            "mae 0.8807769989948099 rmse 3.6151958616617255\n",
            "Log p a-posteriori at iteration 10 : -3733.0922529215286\n",
            "mae 0.08114585122179235 rmse 0.009179188546956847\n",
            "mae 0.8362474456841809 rmse 3.554941842226393\n",
            "Log p a-posteriori at iteration 20 : -2214.5580968426934\n",
            "mae 0.05775608875336177 rmse 0.005388289054980004\n",
            "mae 0.8160951324928959 rmse 3.5326428107867134\n",
            "Log p a-posteriori at iteration 30 : -1581.6044894651936\n",
            "mae 0.04500857803085351 rmse 0.0038965611625478153\n",
            "mae 0.8055154739688309 rmse 3.5226010407753496\n",
            "Log p a-posteriori at iteration 40 : -1242.5500201655311\n",
            "mae 0.03695890701344735 rmse 0.0031569561292444986\n",
            "mae 0.797642911611064 rmse 3.509422927926637\n",
            "Log p a-posteriori at iteration 50 : -1033.4722918052962\n",
            "mae 0.03140377193975204 rmse 0.002737237766451939\n",
            "mae 0.7922820432352955 rmse 3.502354535781701\n",
            "Log p a-posteriori at iteration 60 : -886.1660389179303\n",
            "mae 0.027334255654794633 rmse 0.002474713857967186\n",
            "mae 0.7880980074604362 rmse 3.4954988447676167\n",
            "Log p a-posteriori at iteration 70 : -777.9716536789923\n",
            "mae 0.02422361702440832 rmse 0.0022991922691307934\n",
            "mae 0.7848206845715872 rmse 3.4898309134967396\n",
            "Log p a-posteriori at iteration 80 : -696.7270066579264\n",
            "mae 0.02176663470514875 rmse 0.0021761376999377075\n",
            "mae 0.7823244922014493 rmse 3.4858618155916217\n",
            "Log p a-posteriori at iteration 90 : -633.1800892353729\n",
            "mae 0.019777486339342155 rmse 0.002086432610591704\n",
            "mae 0.7803316545819825 rmse 3.4831968504743025\n",
            "Log p a-posteriori at iteration 100 : -581.7783789867622\n",
            "mae 0.018134309475883962 rmse 0.0020189968527232574\n",
            "mae 0.7787685920634041 rmse 3.4808130036906086\n",
            "Log p a-posteriori at iteration 110 : -539.7911993976221\n",
            "mae 0.016754057734628666 rmse 0.0019670809367151225\n",
            "mae 0.7775263977887668 rmse 3.479746029799774\n",
            "Log p a-posteriori at iteration 120 : -505.01269025634997\n",
            "mae 0.015577484837548216 rmse 0.0019262682086216083\n",
            "mae 0.776477604863784 rmse 3.4793270355607624\n",
            "Log p a-posteriori at iteration 130 : -475.42216902490605\n",
            "mae 0.014562655522892607 rmse 0.0018935684761431193\n",
            "mae 0.7755602674903838 rmse 3.47923051311312\n",
            "Log p a-posteriori at iteration 140 : -449.3598954713628\n",
            "mae 0.013678502363513386 rmse 0.0018669269755706482\n",
            "mae 0.7747603297139852 rmse 3.479274253566911\n",
            "Log p a-posteriori at iteration 150 : -425.65615797654743\n"
          ]
        }
      ],
      "source": [
        "log_ps, rmse_train, rmse_test = train(150, user_features_train, user_features_test)   ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKquVKCh5yGT"
      },
      "outputs": [],
      "source": [
        "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "plt.title('Training results')\n",
        "ax1.plot(np.arange(len(log_ps)), log_ps, label='MAP')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(np.arange(len(rmse_train)), rmse_train, label='RMSE train')\n",
        "ax2.plot(np.arange(len(rmse_test)), rmse_test, label='RMSE test')\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRKrlxft509c",
        "outputId": "d9313874-358a-489a-e50a-84d90ae0c94d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mae 0.19970311350209957 rmse 0.1289247175134238\n",
            "RMSE of training set: 0.3590608827391586\n",
            "mae 0.9291509301265921 rmse 2.5314017249292347\n",
            "RMSE of testing set: 1.5910379395002605\n"
          ]
        }
      ],
      "source": [
        "print('RMSE of training set:', evaluate(train_set, user_features_train))\n",
        "print('RMSE of testing set:', evaluate(test_set, user_features_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1w56mT6_LfS"
      },
      "outputs": [],
      "source": [
        "# amazon_movies\n",
        "RMSE of training set: 4.547824571521348\n",
        "RMSE of testing set: 4.428976555970164"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8RsuYQWwx0d",
        "outputId": "88ae25ed-6279-4492-8554-002be88f7fec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.717906271512138,\n",
              " 2.1908230464091267,\n",
              " 2.1838680034694447,\n",
              " 2.18200862070802,\n",
              " 2.1811151288994233,\n",
              " 2.1804271788358207,\n",
              " 2.179750123552533,\n",
              " 2.1790497853093656,\n",
              " 2.17841697917851,\n",
              " 2.177901897141428,\n",
              " 2.1774771992475954,\n",
              " 2.177123424493576,\n",
              " 2.1768278356652093,\n",
              " 2.1765807730094684,\n",
              " 2.1763739843915357,\n",
              " 2.1761954772126346]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "rmse_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBUZngldi_WW",
        "outputId": "3c4424b5-c580-4b4a-c772-5faae236a455"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.640758457769598,\n",
              " 0.19445396005030074,\n",
              " 0.12115428247808824,\n",
              " 0.09819568547917262,\n",
              " 0.08795667070273773,\n",
              " 0.08254120601665235,\n",
              " 0.0793591721139332,\n",
              " 0.07734787590764923,\n",
              " 0.07600624221639128,\n",
              " 0.07507360671790234,\n",
              " 0.07440378572402324,\n",
              " 0.07390977547111628,\n",
              " 0.07353729198368637,\n",
              " 0.07325114500097318,\n",
              " 0.07302777547305278,\n",
              " 0.07285097264121854]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "rmse_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGhA684xYZqQ"
      },
      "outputs": [],
      "source": [
        "# with personality\n",
        "RMSE of training set: 0.8434822554410405\n",
        "RMSE of testing set: 1.0651675581716595"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMqOrznXX73I"
      },
      "outputs": [],
      "source": [
        "tr[['userId', 'open', 'cons', 'extra', 'agree', 'neuro']].groupby(by='userId').mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rraQxcBYDuK",
        "outputId": "299176f2-0cb4-472d-bcb3-283e83a5118c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.67193 0.42623 0.24570000000000003\n"
          ]
        }
      ],
      "source": [
        "print(max(tr['open']), min(tr['open']), max(tr['open'])-min(tr['open']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drGBFe1ccnNB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ob0hmZjqdnRU",
        "XLSIbDYKuuK8",
        "KBlJab83wB45"
      ],
      "name": "Cross-domain recommendation with user personality.ipynb",
      "provenance": [],
      "mount_file_id": "11nX6xsDctfZUVkUL502Pe0PRMfKvPViB",
      "authorship_tag": "ABX9TyMGTuenpP76GcnqPDBDt1f7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}