{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa06Haw4ixoL/PwzbB1kDR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomdeepAcharyya/Recommender-Systems/blob/main/Personality%20Enabled%20MF/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing of the reviews from the json files and store in csv files\n",
        "# input file : json files from amazon \n",
        "# Tripadvisor dataset need to be preprocessed as well"
      ],
      "metadata": {
        "id": "jrdJDLYCIOZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCEuFXBpHSDU"
      },
      "outputs": [],
      "source": [
        "# importig packages\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import time\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "import scipy.spatial as spt\n",
        "import statistics\n",
        "import math\n",
        "from scipy import spatial\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing packages\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY,p.OPT.MENTION,p.OPT.HASHTAG, p.OPT.ESCAPE_CHAR, p.OPT.RESERVED)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from textblob import Word\n",
        "import re\n",
        "punctuation = re.compile(r'[-.?&!,:;()|0-9]')"
      ],
      "metadata": {
        "id": "tO4Xa4GQHarJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file =  r'/content/drive/MyDrive/Per_CD_RS/Amazon_Text_Magazine_Subscriptions.json'"
      ],
      "metadata": {
        "id": "9mAgwld_HcWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing \n",
        "\n",
        "with open(input_file) as f:\n",
        "    lines = f.read().splitlines()  # split into different lines\n",
        "df_inter = pd.DataFrame(lines)\n",
        "df_inter.columns = ['json_element']\n",
        "df_inter['json_element'].apply(json.loads)\n",
        "az = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
        "\n",
        "arr1 = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})\n",
        "x = pd.DataFrame(arr1.userId.value_counts()).reset_index()\n",
        "y = x[x['userId']>=5]\n",
        "arr2 = pd.DataFrame(y)\n",
        "arr2 = arr2.rename(columns={\"userId\":\"count\",\"index\":\"userId\"})\n",
        "df = pd.merge(arr1, arr2, on='userId', how='inner')\n",
        "df = df[df['rating']<6]\n",
        "df = df[df['rating']>0]\n",
        "x = pd.DataFrame(df.itemId.value_counts()).reset_index()\n",
        "y = x[x['itemId']>=5]\n",
        "arr3 = pd.DataFrame(y)\n",
        "arr3 = arr3.rename(columns={\"itemId\":\"count\",\"index\":\"itemId\"})\n",
        "df2 = pd.merge(df, arr3, on='itemId', how='inner')\n",
        "az = df2\n",
        "tgt = az\n",
        "\n",
        "tgt['processed_text'] = \"\"\n",
        "tgt['reviewText'].fillna(\" \")\n",
        "array_text = []\n",
        "for i in range(len(tgt)):\n",
        "  x = tgt['reviewText'][i]\n",
        "  word_tokens = word_tokenize(x) if type(x) != float else  \" \"\n",
        "  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  lemma_words = []\n",
        "  for words in filtered_sentence:\n",
        "      word = Word(words).lemmatize()\n",
        "      lemma_words.append(word)\n",
        "  punc_words = []\n",
        "  for words in lemma_words:\n",
        "      word =  punctuation.sub(\"\", words)\n",
        "      if len(word) > 0:\n",
        "        punc_words.append(word.lower())\n",
        "  line = \"\"\n",
        "  for i in punc_words:\n",
        "    line = line + \" \" + i.lower()\n",
        "  array_text.append(line)\n",
        "\n",
        "tgt['processed_text'] = array_text\n",
        "az = tgt[['rating', 'userId', 'itemId', 'reviewText', 'processed_text']]\n",
        "az = az.drop_duplicates(subset=None, keep='first')"
      ],
      "metadata": {
        "id": "pjyihYDeHh4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.to_csv(r'/content/drive/MyDrive/Per_CD_RS/Amazon_magazine_filtered.csv')"
      ],
      "metadata": {
        "id": "BNVKyClVHunT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}