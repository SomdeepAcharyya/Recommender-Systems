{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomdeepAcharyya/Recommender-Systems/blob/main/Personality%20Enabled%20MF/Personality%20prediction%20from%20text%20using%20bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSi-GoU33Rq3"
      },
      "outputs": [],
      "source": [
        "# source to be clustered with dbscan/kmeans\n",
        "# target instances to be merged with clusters from source\n",
        "# joint training and embedding on individual subsets\n",
        "# prediction on target sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9srmKd4S4buV"
      },
      "outputs": [],
      "source": [
        "# importing the packages\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import time\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "import scipy.spatial as spt\n",
        "import statistics\n",
        "import math\n",
        "from scipy import spatial\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEDbXqqA5oSq"
      },
      "source": [
        "# data collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW2g_Fhr4dSd"
      },
      "outputs": [],
      "source": [
        "# Tripadvisor review Dataset\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/tripadvisor_reviews_with_country.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  tr = pd.read_csv(infile)\n",
        "tr = tr.rename(columns={\"username\":\"userId\", \"taObject\":\"itemId\"})\n",
        "arr = tr[['open', 'cons', 'extra', 'agree', 'neuro', 'userId', 'itemId', 'rating', 'processed_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDbLDtw74hAa"
      },
      "outputs": [],
      "source": [
        "# amazon review dataset magazine json\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/Amazon_magazine_filtered.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  az = pd.read_csv(infile)\n",
        "#az = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})\n",
        "az.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GC377yL5mxH"
      },
      "outputs": [],
      "source": [
        "tr1 = arr[0:90]\n",
        "tr2 = arr[90:100]\n",
        "source_texts = np.array(tr1.processed_text)\n",
        "target_texts = np.array(tr2.processed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCpVdSl29tyt"
      },
      "source": [
        "# Clustering of text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcntDUMx9wUp"
      },
      "outputs": [],
      "source": [
        "# clustering of sorce domain reviews \n",
        "\n",
        "def cluster_text(X):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.cluster import KMeans\n",
        "    Sum_of_squared_distances = []\n",
        "    K = range(2,20)\n",
        "    for k in K:\n",
        "       km = KMeans(n_clusters=k, max_iter=200, n_init=5)\n",
        "       km = km.fit(X)\n",
        "       Sum_of_squared_distances.append(km.inertia_)\n",
        "    plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('Sum_of_squared_distances')\n",
        "    plt.title('Elbow Method For Optimal k')\n",
        "    plt.show()\n",
        "\n",
        "    print('How many clusters do you want to use?')\n",
        "    no_of_clusters = int(input())\n",
        "    model = KMeans(n_clusters=no_of_clusters, init='k-means++', max_iter=200, n_init=10)\n",
        "    model.fit(X)\n",
        "\n",
        "    labels=model.labels_\n",
        "    clusters=pd.DataFrame(list(zip(X,labels)),columns=['title','cluster'])\n",
        "\n",
        "    for i in range(no_of_clusters):\n",
        "        print(clusters[clusters['cluster'] == i])\n",
        "        \n",
        "    return clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ISmS7kB-UXa"
      },
      "outputs": [],
      "source": [
        "# remove nan values from source and target domains\n",
        "s = tr1.processed_text.fillna(\" \")\n",
        "t = tr2.processed_text.fillna(\" \")\n",
        "print(True in np.array(t.isna()))\n",
        "print(True in np.array(s.isna()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_mKhbSSBViT"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features = 100) \n",
        "vectorizer.fit(s) \n",
        "source_embeddings = vectorizer.transform(s)\n",
        "vectorizer.fit(t) \n",
        "target_embeddings = vectorizer.transform(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gaGfcFXEIUf"
      },
      "outputs": [],
      "source": [
        "c = cluster_text(source_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umQ5emYTry-a"
      },
      "outputs": [],
      "source": [
        "# to store the source domain clusters for future usage\n",
        "c.to_csv(r'/content/drive/MyDrive/Per_CD_RS/tripcluster.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPc6fP7p_Ac-"
      },
      "outputs": [],
      "source": [
        "# source domain cluster Dataset read from earlier stage\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/tripcluster.csv'\n",
        "with open(path, errors='ignore') as infile:\n",
        "  d = pd.read_csv(infile)\n",
        "#d.title = source_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c.cluster.value_counts()"
      ],
      "metadata": {
        "id": "vzSpHXMgw8K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1e8udXkbLzu"
      },
      "outputs": [],
      "source": [
        "set_source = []\n",
        "no_clusters = len(c.cluster.value_counts())\n",
        "for k in range(0, no_clusters):\n",
        "  set_source.append(np.array(c[c['cluster'] == k].title))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKStSX9bWjwg"
      },
      "outputs": [],
      "source": [
        "# similarity coeff v1: using temperature parameter\n",
        "li = []\n",
        "temp = 100\n",
        "li_total = []\n",
        "counter = 0\n",
        "for i in target_embeddings:\n",
        "  if counter % 200 == 0:\n",
        "    print(counter)\n",
        "  for j in range(len(set_source)):\n",
        "    sim = []\n",
        "    for k in set_source[j]:\n",
        "      val1 = math.exp(1 - spatial.distance.cosine(i.toarray(), k.toarray()) / temp)\n",
        "      sim.append(val1)\n",
        "    denom = sum(sim)  \n",
        "    for k in sim: \n",
        "      val2 = -1 * math.log(k) / denom\n",
        "      li.append(val2)\n",
        "    li_total.append(sum(li) / len(li))\n",
        "    li = []\n",
        "  counter = counter + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCsZxOXrTZdc"
      },
      "outputs": [],
      "source": [
        "# similarity coeff v2: using normalised version with temperature parameter\n",
        "li = []\n",
        "denom = []\n",
        "temp = 0.2\n",
        "li_total = []\n",
        "for i in target_embeddings:\n",
        "  for j in range(len(set_source)):\n",
        "    sim = []\n",
        "    for k in set_source[j]:\n",
        "      val = math.exp(1 - spatial.distance.cosine(i.toarray(), k.toarray()) / temp)\n",
        "      sim.append(val)\n",
        "    denom.append(sum(sim))\n",
        "  for j in range(len(set_source)):\n",
        "    for k in set_source[j]:\n",
        "      val = -1 * math.log(math.exp(1 - spatial.distance.cosine(i.toarray(), k.toarray()) / temp) / denom[j])\n",
        "      li.append(val)\n",
        "    li_total.append(sum(li) / len(li))\n",
        "    li = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ8m8KTX-9aB"
      },
      "outputs": [],
      "source": [
        "# similarity coeff v3: cosine similarity\n",
        "li = []\n",
        "li_total = []\n",
        "counter = 0\n",
        "for i in target_embeddings:\n",
        "  if counter % 200 == 0:\n",
        "    print(counter)\n",
        "  for j in range(len(set_source)):\n",
        "    sim = []\n",
        "    for k in set_source[j]:\n",
        "      val1 = 1 - spatial.distance.cosine(i.toarray().flatten(), k.toarray().flatten())\n",
        "      sim.append(val1)\n",
        "    li_total.append(sum(sim) / len(sim))\n",
        "    li = []\n",
        "  counter = counter + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6sITi-Mw8Zo"
      },
      "outputs": [],
      "source": [
        "li_all = np.reshape(li_total, (target_embeddings.shape[0], int(len(li_total)/target_embeddings.shape[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cVdRUDGdCgb"
      },
      "outputs": [],
      "source": [
        "index_of_cluster = []\n",
        "for i in li_all:\n",
        "  i = list(i) \n",
        "  index = i.index(min(i))\n",
        "  index_of_cluster.append(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEQAIOKlddjJ"
      },
      "outputs": [],
      "source": [
        "source_clusters = c\n",
        "target_clusters = pd.DataFrame(list(zip(target_embeddings,index_of_cluster, tr2.rating, tr2.userId)),columns=['title','cluster', 'rating', 'userId'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdCSJbe4drlh"
      },
      "outputs": [],
      "source": [
        "source_clusters['text'] = source_texts\n",
        "target_clusters['text'] = target_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxNEDN-rIeb3"
      },
      "outputs": [],
      "source": [
        "target_clusters.cluster.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhXBg3pHaET0"
      },
      "outputs": [],
      "source": [
        "# store target domain clusters for future usage\n",
        "target_clusters.to_csv(r'/content/drive/MyDrive/Per_CD_RS/magz_clusters1.csv') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GpA2XZcL6dy"
      },
      "outputs": [],
      "source": [
        "# target domain cluster Dataset read from earlier stage\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/magz_clusters1.csv'\n",
        "with open(path, errors='ignore') as infile:\n",
        "  target_clusters= pd.read_csv(infile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_7gSCr9gso4"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ov6UbjXaeuhC"
      },
      "outputs": [],
      "source": [
        "!pip install simpletransformers\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvUjlOn-gxCO"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ndDdf_Rg68O"
      },
      "outputs": [],
      "source": [
        "model_args = ClassificationArgs()\n",
        "model_args.num_train_epochs = 1\n",
        "model_args.regression = True\n",
        "model_args.overwrite_output_dir = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThWkEclng-Mr"
      },
      "outputs": [],
      "source": [
        "pers_s = np.array(tr1[['open', 'cons', 'extra', 'agree', 'neuro']])\n",
        "source_clusters['userId'] = np.array(tr1['userId'])\n",
        "new_s = source_clusters.merge(tr1[['userId', 'open', 'cons', 'extra', 'agree', 'neuro']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6THvPvaPhBVw"
      },
      "outputs": [],
      "source": [
        "# prediction of personality scores using bert model\n",
        "\n",
        "target_output = pd.DataFrame()\n",
        "target = pd.DataFrame()\n",
        "traits = [\"open\", \"cons\", \"extra\", \"agree\", \"neuro\"]\n",
        "\n",
        "for i in range(0, no_clusters):\n",
        "  t = pd.DataFrame()\n",
        "  t['userId'] = z['userId'][lx:len(z)]\n",
        "  t['open'] = 0.0\n",
        "  t['cons'] = 0.0\n",
        "  t['extra'] = 0.0\n",
        "  t['agree'] = 0.0\n",
        "  t['neuro'] = 0.0\n",
        "  for tr in traits:\n",
        "    x = new_s[new_s['cluster']==i]\n",
        "    y = target_clusters[target_clusters['cluster']==i]\n",
        "    z = pd.concat([x,y])\n",
        "    lx = len(x)\n",
        "    ly = len(y)\n",
        "    s = tr\n",
        "    z = z.fillna(0)\n",
        "    for j in range(len(s)):\n",
        "      try:\n",
        "        sample = z[['text', s[j]]]\n",
        "        a = sample[0:lx]\n",
        "        b = sample[lx:len(sample)]\n",
        "        if len(b) > 0:\n",
        "          model = ClassificationModel(\"bert\", \"bert-base\", num_labels=1, args=model_args, use_cuda=False)\n",
        "          print(\"1\")\n",
        "          model.train_model(a)\n",
        "          print(\"2\")\n",
        "          result, model_outputs, wrong_predictions = model.eval_model(b)\n",
        "          print(\"3\")\n",
        "          print(result, model_outputs, wrong_predictions)\n",
        "          print(\"4\")\n",
        "          t[s[j]] = model_outputs\n",
        "        else:\n",
        "          print(i, j, \"b empty\")\n",
        "      except:\n",
        "            print(\"error\")\n",
        "    target_output = pd.concat([target_output, t])\n",
        "    target[tr] = target_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#neuro = target_output "
      ],
      "metadata": {
        "id": "RrDrY-MMpKFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#target['open'] = open.open\n",
        "#target['cons'] = cons.cons\n",
        "#target['extra'] = extra.extra\n",
        "#target['agree'] = agree.agree\n",
        "#target['neuro'] = neuro.neuro\n",
        "#target['userId'] = cons.userId"
      ],
      "metadata": {
        "id": "uBdOvmcDT8kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8c2bG01hWzT"
      },
      "outputs": [],
      "source": [
        "# store the personality scores of target domain users\n",
        "target.to_csv(r'/content/drive/MyDrive/Per_CD_RS/magazine_output_final.csv') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Combine"
      ],
      "metadata": {
        "id": "7-sDnpYyK4Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combining the personality scores across all clusters"
      ],
      "metadata": {
        "id": "X6V3iEBoFQvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = r'/content/drive/MyDrive/Per_CD_RS/magazine_output_final.csv' \n",
        "\n",
        "with open(path1, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  df = pd.read_csv(infile)\n",
        "#arr = np.array(df[['0', '1', '2', '3', '4']])\n",
        "\n",
        "# amazon review dataset movies json\n",
        "path2 = r'/content/drive/MyDrive/Per_CD_RS/Amazon_magazine_filtered.csv'\n",
        "with open(path2, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  az = pd.read_csv(infile, nrows=100)"
      ],
      "metadata": {
        "id": "IPJTT5CAOtiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "jDk5Z5baMR5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az"
      ],
      "metadata": {
        "id": "vt33Fed2MS-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = pd.merge(df, az, on='userId', how='inner')"
      ],
      "metadata": {
        "id": "7QjQnPwaMUFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t"
      ],
      "metadata": {
        "id": "7VFx2wesN_oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ko4GN1yOeDp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1p5WEQrZzJdo6p8kvrATWdqLf7vERLiRq",
      "authorship_tag": "ABX9TyNtO7Bv35SymiQVPpr2Oxno",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}