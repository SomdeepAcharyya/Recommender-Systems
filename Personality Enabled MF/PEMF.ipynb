{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomdeepAcharyya/Recommender-Systems/blob/main/Personality%20Enabled%20MF/PEMF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08IHqa4pgM8h"
      },
      "outputs": [],
      "source": [
        "# PEMF MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kQGxLXcXklX6"
      },
      "outputs": [],
      "source": [
        "# importing packages\n",
        "import numpy as np\n",
        "import numba as nb\n",
        "import scipy.sparse as sp\n",
        "import time\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import scipy.spatial as spt\n",
        "import statistics\n",
        "import math\n",
        "import json\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from random import sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OnPMs77gKxJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085f159d-805e-435b-f869-1772c52ea39e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# importing packages\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY,p.OPT.MENTION,p.OPT.HASHTAG, p.OPT.ESCAPE_CHAR, p.OPT.RESERVED)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from textblob import Word\n",
        "import re\n",
        "punctuation = re.compile(r'[-.?&!,:;()|0-9]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7S7t-idOyXui"
      },
      "outputs": [],
      "source": [
        "def pemf(R,ul,il,S,N,M,K,lambdaU,lambdaV,lambdaT,R_test,ult,ilt,St):\n",
        "    def sigmoid(z):\n",
        "        return 1 + ((5.0 - 1.0) / (1+np.exp(-z)))\n",
        "    def dsigmoid(z):\n",
        "        return (5.0 - 1.0) * np.exp(-z)/np.power((1+np.exp(-z)),2)\n",
        "    def rmse(U,V,R):\n",
        "        keylist = []\n",
        "        dok_keys = np.array(R.todok().keys()).T.reshape([1,1])[0][0]\n",
        "        for k in dok_keys:\n",
        "          row = []\n",
        "          for l in k:\n",
        "            row.append(l)\n",
        "          keylist.append(row) \n",
        "        keylist = np.array(keylist)\n",
        "        utl = keylist[:, 0]\n",
        "        itl = keylist[:, 1]\n",
        "        error = (get_csrmat(sigmoid(U.dot(V.T)),utl,itl)-R).power(2).sum()/R.nnz\n",
        "        error = np.sqrt((get_csrmat(sigmoid(U.dot(V.T)),utl,itl)-R).power(2)).sum()/np.sqrt(R.nnz)\n",
        "        return np.sqrt(error)\n",
        "    def mae(U,V,R):\n",
        "        keylist = []\n",
        "        dok_keys = np.array(R.todok().keys()).T.reshape([1,1])[0][0]\n",
        "        for k in dok_keys:\n",
        "          row = []\n",
        "          for l in k:\n",
        "            row.append(l)\n",
        "          keylist.append(row) \n",
        "        keylist = np.array(keylist)\n",
        "        utl = keylist[:, 0]\n",
        "        itl = keylist[:, 1]\n",
        "        error = abs(get_csrmat(sigmoid(U.dot(V.T)),utl,itl)-R).sum()/R.nnz \n",
        "        return error\n",
        "    def get_csrmat(mat,ul,il):\n",
        "        indx = ul*mat.shape[1]+il\n",
        "        return sp.csr_matrix((np.take(np.array(mat),indx),(ul,il)),shape=(mat.shape[0],mat.shape[1]))\n",
        "    def costL(U,V,S,ul,il):\n",
        "        tmp = U.dot(V.T)\n",
        "        Rx = get_csrmat(sigmoid(tmp),ul,il)\n",
        "        cost = 0.5*((R - Rx).power(2)).sum()+0.5*lambdaU*np.linalg.norm(U)**2+0.5*lambdaV*np.linalg.norm(V)**2\n",
        "        cost += 0.5*lambdaT*np.power(U-S.dot(U),2).sum()\n",
        "        return cost\n",
        "    def gradient(U,V,S,ul,il):\n",
        "        dU = np.zeros(U.shape)\n",
        "        dV = np.zeros(V.shape)\n",
        "        dU = lambdaU*U\n",
        "        tmp = U.dot(V.T)\n",
        "        Rv = get_csrmat(dsigmoid(tmp),ul,il)\n",
        "        Rx = get_csrmat(sigmoid(tmp),ul,il)\n",
        "        dU += Rv.multiply((Rx-R)).dot(V)\n",
        "        dU += lambdaT*(U-S.dot(U))-lambdaT*S.T.dot((U-S.dot(U)))\n",
        "        dV = lambdaV*V\n",
        "        dV += (Rv.multiply((Rx-R))).T.dot(U)\n",
        "        if np.max(dU)>1:\n",
        "            dU = dU/np.max(dU)\n",
        "        if np.max(dV)>1:\n",
        "            dV = dV/np.max(dV)\n",
        "        return dU,dV\n",
        "\n",
        "\n",
        "    def train(U,V,S,ul,il):\n",
        "        res=[]\n",
        "        steps=10\n",
        "        rate = 1e-2\n",
        "        pregradU = 0\n",
        "        pregradV = 0\n",
        "        tol=1e-6\n",
        "        momentum = 0.9\n",
        "        stage = max(steps/100 , 1)\n",
        "        for step in range(steps):\n",
        "            start = time.time()\n",
        "            dU,dV = gradient(U,V,S,ul,il)\n",
        "            dU = dU + momentum*pregradU\n",
        "            dV = dV + momentum*pregradV\n",
        "            pregradU = dU\n",
        "            pregradV = dV\n",
        "            if not step%stage and rate>0.001:\n",
        "                rate = 0.95*rate\n",
        "            U -= rate * dU\n",
        "            V -= rate * dV\n",
        "            e = costL(U,V,S,ul,il) / (len(U) * len(V))\n",
        "            res.append(e)\n",
        "            if not step%stage:\n",
        "                #print(\"step\",step,\"error\", e, \"time\", time.time() - start)\n",
        "                #print(\"RMSE\",rmse(U,V,R), \"MAE\",mae(U,V,R))\n",
        "                e1 = e\n",
        "            if step>150:# or abs(sum(res[-3:])-sum(res[-13:-10]))<tol:\n",
        "                #print(\"====================\")\n",
        "                #print(\"stop in %d step\"%(step))\n",
        "                #print(\"error is \",e)\n",
        "                #print(\"====================\")\n",
        "                break\n",
        "        return U, V\n",
        "\n",
        "\n",
        "    U = np.random.normal(0,0.01,size=(R.shape[0],K))\n",
        "    V = np.random.normal(0,0.01,size=(R.shape[1],K))\n",
        "    Ut = np.random.normal(0,0.01,size=(R_test.shape[0],K))\n",
        "    Vt = np.random.normal(0,0.01,size=(R_test.shape[1],K))\n",
        "    start = time.time()\n",
        "    U,V = train(U,V,S,ul,il)\n",
        "\n",
        "    #print(\"=================RESULT=======================\")\n",
        "    #print('K:%d,lambdaU:%s, lambdaV:%s,lambdaT:%s' \\\n",
        "    #        %(K,lambdaU,lambdaV,lambdaT))\n",
        "    #print(\"mae\",mae(U,V,R_test))\n",
        "    #print(\"time\",time.time() - start)\n",
        "    #print(\"========================================\")\n",
        "    #print(\"mae recal\", mean_absolute_error((U@V.T).toarray(), R_test))\n",
        "\n",
        "    print(\"rmse \",rmse(Ut,Vt,R_test), \"mae \",mae(Ut,Vt,R_test), \"time\",time.time() - start)\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_distance(A,A1):\n",
        "  similarity = np.dot(A, A1.T)\n",
        "  square_mag = np.diag(similarity)\n",
        "  inv_square_mag = 1 / square_mag\n",
        "  inv_square_mag[np.isinf(inv_square_mag)] = 0\n",
        "  inv_mag = np.sqrt(inv_square_mag)\n",
        "  cosine = similarity * inv_mag\n",
        "  cosine = cosine.T * inv_mag\n",
        "  return cosine"
      ],
      "metadata": {
        "id": "BxVJNZOLxc-n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoh5U_kVLnio"
      },
      "source": [
        "# target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data"
      ],
      "metadata": {
        "id": "S2oEa-sToZoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cpFHYiuBWgC7"
      },
      "outputs": [],
      "source": [
        "# Tripadvisor review Dataset\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/tripadvisor_reviews_with_country.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  tr = pd.read_csv(infile)\n",
        "tr = tr.rename(columns={\"username\":\"userId\", \"taObject\":\"itemId\"})\n",
        "arr = tr[['open', 'cons', 'extra', 'agree', 'neuro', 'userId', 'itemId', 'rating']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MoYNhrun4QC9"
      },
      "outputs": [],
      "source": [
        "# amazon review dataset magazines csv\n",
        "path1 = r'/content/drive/MyDrive/Per_CD_RS/magazine_output_final.csv'\n",
        "with open(path1, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  df = pd.read_csv(infile, nrows=100)\n",
        "#arr = np.array(df[['0', '1', '2', '3', '4']])\n",
        "\n",
        "#path2 = r'/content/drive/MyDrive/Per_CD_RS/Aaamzon_fashion_ru_tf.csv'\n",
        "#with open(path2, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "#  ru_tf_df = pd.read_csv(infile)\n",
        "#ru_tf_df = ru_tf_df.drop(columns=['Unnamed: 0'])\n",
        "#ru_tf = np.array(ru_tf_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# amazon review dataset movies json\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/Amazon_magazine_filtered.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  az = pd.read_csv(infile)\n",
        "#az = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})"
      ],
      "metadata": {
        "id": "NrpayR6tMfBI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = pd.merge(df, az, on='userId', how='inner')\n",
        "arr = t[['open', 'cons', 'extra', 'agree', 'neuro', 'userId', 'itemId', 'rating']]\n",
        "t.shape"
      ],
      "metadata": {
        "id": "WPV1t1ZcKhdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d521428-cbc1-4a9f-b65b-d747c8a14743"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(557, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocessing"
      ],
      "metadata": {
        "id": "Aq9ugeI8SDnS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOVLqNxmDUR2"
      },
      "outputs": [],
      "source": [
        "input_file =  r'/content/drive/MyDrive/Per_CD_RS/Amazon_Text_Video_Games.json'\n",
        "with open(input_file) as f:\n",
        "    lines = f.read().splitlines()\n",
        "df_inter = pd.DataFrame(lines)\n",
        "df_inter.columns = ['json_element']\n",
        "df_inter['json_element'].apply(json.loads)\n",
        "az = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
        "\n",
        "arr1 = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})\n",
        "x = pd.DataFrame(arr1.userId.value_counts()).reset_index()\n",
        "y = x[x['userId']>=5]\n",
        "arr2 = pd.DataFrame(y)\n",
        "arr2 = arr2.rename(columns={\"userId\":\"count\",\"index\":\"userId\"})\n",
        "df = pd.merge(arr1, arr2, on='userId', how='inner')\n",
        "df = df[df['rating']<6]\n",
        "df = df[df['rating']>0]\n",
        "x = pd.DataFrame(df.itemId.value_counts()).reset_index()\n",
        "y = x[x['itemId']>=5]\n",
        "arr3 = pd.DataFrame(y)\n",
        "arr3 = arr3.rename(columns={\"itemId\":\"count\",\"index\":\"itemId\"})\n",
        "df2 = pd.merge(df, arr3, on='itemId', how='inner')\n",
        "az = df2\n",
        "tgt = az\n",
        "\n",
        "tgt['processed_text'] = \"\"\n",
        "tgt['reviewText'].fillna(\" \")\n",
        "array_text = []\n",
        "for i in range(len(tgt)):\n",
        "  x = tgt['reviewText'][i]\n",
        "  word_tokens = word_tokenize(x) if type(x) != float else  \" \"\n",
        "  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  lemma_words = []\n",
        "  for words in filtered_sentence:\n",
        "      word = Word(words).lemmatize()\n",
        "      lemma_words.append(word)\n",
        "  punc_words = []\n",
        "  for words in lemma_words:\n",
        "      word =  punctuation.sub(\"\", words)\n",
        "      if len(word) > 0:\n",
        "        punc_words.append(word.lower())\n",
        "  line = \"\"\n",
        "  for i in punc_words:\n",
        "    line = line + \" \" + i.lower()\n",
        "  array_text.append(line)\n",
        "\n",
        "tgt['processed_text'] = array_text\n",
        "az = tgt[['rating', 'userId', 'itemId', 'reviewText', 'processed_text']]\n",
        "az = az.drop_duplicates(subset=None, keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "az = az[0:len(arr)]"
      ],
      "metadata": {
        "id": "f36zI8UonE3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqTOfAhLJH7i"
      },
      "outputs": [],
      "source": [
        "# amazon review dataset movies json\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/Amazon_fashion_filtered.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  az = pd.read_csv(infile)\n",
        "az = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LnDQNXHguUs"
      },
      "outputs": [],
      "source": [
        "# amazon review dataset magazines csv\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/Amazon_Text_Movies_and_TV.json'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  az = pd.read_json(infile, lines=True)\n",
        "az = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37yzIrr3hB2V"
      },
      "outputs": [],
      "source": [
        "src = az[['userId', 'itemId', 'rating']]\n",
        "arr = pd.DataFrame(arr)\n",
        "arr['userId'] = az['userId']\n",
        "arr['itemId'] = np.array(az['itemId'])\n",
        "arr['rating'] = az['rating']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## matrix factorization"
      ],
      "metadata": {
        "id": "57DiP3NrSIqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CoEu4fwZhLL6"
      },
      "outputs": [],
      "source": [
        "# Construction of rating matrix wrt user u\n",
        "ru = arr.pivot_table(index='userId',columns='itemId',values='rating')\n",
        "ru = ru.fillna(0)\n",
        "ru_m = ru > 0\n",
        "ru_m = ru_m.replace(True, 1)\n",
        "ru_m = ru_m.replace(False, 0)\n",
        "ru = np.array(ru)\n",
        "ru_m = np.array(ru_m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "L8UipsLlMESh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574c6b28-7714-4265-e03a-d7861d6f35fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(94, 187)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "ru = arr.groupby(['userId', 'itemId'])['rating'].sum().unstack()\n",
        "ru = pd.DataFrame(ru)\n",
        "ru.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dividing into train and test\n",
        "\n",
        "def split_data(ru, s, k):\n",
        "  split = s\n",
        "  k = k\n",
        "  test = []\n",
        "  set1  = ru.iloc[: , :int(ru.shape[1]/split)]\n",
        "  set1 = set1.replace(np.nan, 0)\n",
        "  for i in range(k):\n",
        "    set2 = ru.iloc[: , i*int(ru.shape[1]/split):(i+1)*int(ru.shape[1]/split)]\n",
        "    set2 = set2.replace(np.nan, 0)\n",
        "    set1 = pd.concat([set1, set2], axis=1)\n",
        "    set1  = ru.iloc[: , :int(ru.shape[1]/split)]\n",
        "    set1 = set1.replace(np.nan, 0)\n",
        "    set2  = ru.iloc[: , int(ru.shape[1]/split): 2*int(ru.shape[1]/split)]\n",
        "    set2 = set2.replace(np.nan, 0)\n",
        "    xx = pd.concat([set1, set2], axis=1)\n",
        "    test.append(xx)\n",
        "  return test"
      ],
      "metadata": {
        "id": "JunMaXAV9bhe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mprzddQjhRfs"
      },
      "outputs": [],
      "source": [
        "# with only trust and rating use this\n",
        "\n",
        "def tgt_pred(l, k, a, lp, t, th, fold, split):\n",
        "\n",
        "    train_size = split\n",
        "    lr, k, alpha, lambda_p, lambda_t, thr = l, k, a, lp, t, th\n",
        "    df_copy = arr.copy()\n",
        "    train_set = df_copy.sample(frac=train_size).reset_index()\n",
        "    user_features_train = np.array(train_set[['open', 'cons', 'extra', 'agree', 'neuro']].fillna(0))\n",
        "    #test_set = df_copy.drop(train_set.index).reset_index()\n",
        "    test_set = pd.DataFrame(split_data(df_copy, (1-train_size)*10, fold)[0])\n",
        "    user_features_test = np.array(test_set[['open', 'cons', 'extra', 'agree', 'neuro']].fillna(0))\n",
        "\n",
        "    df = pd.DataFrame(user_features_train) \n",
        "    df.columns = ['open', 'cons', 'extra', 'agree', 'neuro']\n",
        "    df['userId'] = train_set.userId\n",
        "    df2 = df.groupby(by='userId').mean().reset_index()\n",
        "    df3 = np.array(df2[[\"open\", 'cons', 'extra', 'agree', 'neuro']])\n",
        "    \n",
        "\n",
        "    \n",
        "    ru = train_set.groupby(['userId', 'itemId'])['rating'].sum().unstack()\n",
        "    ru = pd.DataFrame(ru)\n",
        "    set1 = ru.iloc[: , :int(ru.shape[1]/(1-train_size)*10)]\n",
        "    set1 = set1.replace(np.nan, 0)\n",
        "    #for i in range(1,2):\n",
        "    # set2 = ru.iloc[: , i*int(ru.shape[1]/10):(i+1)*int(ru.shape[1]/10)]\n",
        "    #  set2 = set2.replace(np.nan, 0)\n",
        "    #  set1 = pd.concat([set1, set2], axis=1)\n",
        "    ru = set1\n",
        "    ru_m = ru > 0\n",
        "    ru_m = ru_m.replace(True, 1)\n",
        "    ru_m = ru_m.replace(False, 0)\n",
        "    ru = np.array(ru)\n",
        "    ru_m = np.array(ru_m)\n",
        "\n",
        "    R0 = ru    # rating matrix\n",
        "    max_r = 5.0    # max _rating   \n",
        "    N,M= ru.shape[0],ru.shape[1]     # # n : no of users   m : no of items \n",
        "    K = k                       #k : latent dimension        (mx10)@(10xn)\n",
        "    lambdaU,lambdaV,lambdaT= lambda_p, lambda_p, lambda_t\n",
        "    keys = []\n",
        "\n",
        "    R=sp.dok_matrix((N,M))   # create sparse matrix for user x item\n",
        "    T=sp.dok_matrix((N,N))   # create sparse matrix for trust among users user x user\n",
        "    for i in range(len(R0)):    # no of users\n",
        "        for j in range(len(R0[i])):   # no of items\n",
        "            if R0[i][j]>0:    # if rating is present   \n",
        "                keys.append([i,j])\n",
        "                R[i,j] = R0[i][j]     \n",
        "\n",
        "    dft = pd.DataFrame(user_features_test) \n",
        "    dft.columns = ['open', 'cons', 'extra', 'agree', 'neuro']\n",
        "    dft['userId'] = test_set.userId\n",
        "    df2t = dft.groupby(by='userId').mean().reset_index()\n",
        "    df3t = np.array(df2t[[\"open\", 'cons', 'extra', 'agree', 'neuro']])\n",
        "\n",
        "    rut = test_set.pivot_table(index='userId',columns='itemId',values='rating')\n",
        "    rut = test_set.groupby(['userId', 'itemId'])['rating'].sum().unstack()\n",
        "    rut = pd.DataFrame(rut)\n",
        "    rut = rut.fillna(0)\n",
        "    ru_mt = rut > 0\n",
        "    ru_mt = ru_mt.replace(True, 1)\n",
        "    ru_mt = ru_mt.replace(False, 0)\n",
        "    rut = np.array(rut)\n",
        "    ru_mt = np.array(ru_mt)\n",
        "\n",
        "    R0t = rut    # rating matrix\n",
        "    max_rt = 5.0    # max _rating   \n",
        "    Nt,Mt= rut.shape[0],rut.shape[1]     # # n : no of users   m : no of items \n",
        "    K = k                        #k : latent dimension        (mxk)@(kxn)\n",
        "    lambdaU,lambdaV,lambdaT = lambdaU,lambdaV,lambdaT\n",
        "    keyst = []\n",
        "\n",
        "    Rt = sp.dok_matrix((Nt,Mt))   # create sparse matrix for user x item\n",
        "    Tt = sp.dok_matrix((Nt,Nt))   # create sparse matrix for user x user\n",
        "    for i in range(len(R0t)):    # no of users\n",
        "        for j in range(len(R0t[i])):   # no of items\n",
        "            if R0t[i][j]>0:    # if rating is present   \n",
        "                keyst.append([i,j])\n",
        "                Rt[i,j] = R0t[i][j]      \n",
        "\n",
        "    \n",
        "    # get trust factor between users trust_uv(user x user)  and sim_uv(user x user) t_uv(user x user)\n",
        "\n",
        "\n",
        "    trust_uv = 1 - calc_distance(df3, df3)\n",
        "    trust_uv = np.nan_to_num(trust_uv)\n",
        "    trust_uv_s = trust_uv.copy()\n",
        "    trust_uv = pd.DataFrame(trust_uv >= thr)\n",
        "    trust_uv = trust_uv.replace(True, 1)\n",
        "    trust_uv = trust_uv.replace(False, 0)\n",
        "    trust_uv = np.array(trust_uv)\n",
        "\n",
        "    tknn = []\n",
        "    for j in range(len(trust_uv)):\n",
        "      row = []\n",
        "      for m in range(len(trust_uv[j])):\n",
        "        if trust_uv[j][m] == 1:\n",
        "          row.append(m)\n",
        "      tknn.append(row)\n",
        "    tknn = np.array(tknn)\n",
        "    sim_uv = 1- calc_distance(ru, ru)\n",
        "    sim_uv = np.nan_to_num(sim_uv)\n",
        "    #ru_tf2 = ru_tf[0:trust_uv.shape[0]]\n",
        "    #ru_tf2 = ru_tf[:,:len(trust_uv)]\n",
        "    t_uv = np.add(alpha * trust_uv_s, (1-alpha)* sim_uv)\n",
        "    #t_uv = np.multiply(np.add(alpha * trust_uv_s, (1-alpha)* sim_uv), ru_tf)\n",
        "\n",
        "    max_value = max(t_uv.flatten())\n",
        "    min_value = min(t_uv.flatten())\n",
        "    T_df = pd.DataFrame(t_uv)\n",
        "    T_df = T_df >=  np.subtract(T_df, min_value) /(max_value - min_value)   # thr is threshold value\n",
        "    T_df = T_df.replace(True, 1)\n",
        "    T_df = T_df.replace(False, 0)\n",
        "    T_df = np.array(T_df)\n",
        "    T = sp.csr_matrix(T_df)   # fill up trust matrix from user realtionships   *******\n",
        "\n",
        "\n",
        "    trust_uv_t = 1 - calc_distance(df3t, df3t)\n",
        "    trust_uv_t = np.nan_to_num(trust_uv_t)\n",
        "    trust_uv_st = trust_uv_t.copy()\n",
        "    trust_uv_t = pd.DataFrame(trust_uv_t >= thr)\n",
        "    trust_uv_t = trust_uv_t.replace(True, 1)\n",
        "    trust_uv_t = trust_uv_t.replace(False, 0)\n",
        "    trust_uv_t = np.array(trust_uv_t)\n",
        "\n",
        "    tknn_t = []\n",
        "    for j in range(len(trust_uv_t)):\n",
        "      row = []\n",
        "      for m in range(len(trust_uv_t[j])):\n",
        "        if trust_uv_t[j][m] == 1:\n",
        "          row.append(m)\n",
        "      tknn_t.append(row)\n",
        "    tknn_t = np.array(tknn)\n",
        "    sim_uv_t = 1- calc_distance(rut, rut)\n",
        "    sim_uv_t = np.nan_to_num(sim_uv_t)\n",
        "    #ru_tf2 = ru_tf[0:trust_uv.shape[0]]\n",
        "    #ru_tf2 = ru_tf[:,:len(trust_uv)]\n",
        "    t_uv_t = np.add(alpha * trust_uv_st, (1-alpha)* sim_uv_t)\n",
        "    #t_uv = np.multiply(np.add(alpha * trust_uv_s, (1-alpha)* sim_uv), ru_tf)\n",
        "\n",
        "    max_value = max(t_uv_t.flatten())\n",
        "    min_value = min(t_uv_t.flatten())\n",
        "    T_df_t = pd.DataFrame(t_uv_t)\n",
        "    T_df_t = T_df_t >=  np.subtract(T_df_t, min_value) /(max_value - min_value)   # thr is threshold value\n",
        "    T_df_t = T_df_t.replace(True, 1)\n",
        "    T_df_t = T_df_t.replace(False, 0)\n",
        "    T_df_t = np.array(T_df_t)\n",
        "    Tt = sp.csr_matrix(T_df_t)   # fill up trust matrix from user realtionships   *******\n",
        "\n",
        "\n",
        "    \n",
        "    keys = np.array(keys)        # get the keys\n",
        "    keyst = np.array(keyst)\n",
        "    R,T = R.tocsr(),T.tocsr()\n",
        "    Rt,Tt = Rt.tocsr(), Tt.tocsr()\n",
        "    pemf(R,keys[:, 0],keys[:, 1],T,N,M,K,lambdaU,lambdaV,lambdaT,Rt,keyst[:, 0],keyst[:, 1],Tt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "WqL-UmnioR4R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gQSWNzk40RTL"
      },
      "outputs": [],
      "source": [
        "lr = [0.0001]\n",
        "K = [40]\n",
        "alpha = [0.5]\n",
        "lambda_p = [0.02]\n",
        "lambda_t = [0.5]\n",
        "thres = [0.7]\n",
        "fold = 1\n",
        "split = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ng3F_bKdaLDn"
      },
      "outputs": [],
      "source": [
        "l, k, a, lp, lt, th = 0.0001, 20, 0.5, 0.03, 0.8, 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vW8lXBJal7Se",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648dcd9d-973f-4ce3-a6fb-21e102f6d295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0001 40 0.5 0.02 0.5 0.7\n",
            "rmse  7.51462790680989 mae  2.8779589644793466 time 0.06507492065429688\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  for i in range(fold):\n",
        "    for l in lr:\n",
        "      for k in K:\n",
        "        for a in alpha:\n",
        "          for lp in lambda_p:\n",
        "            for lt in lambda_t:\n",
        "              for th in thres:\n",
        "                print(l, k, a, lp, lt, th)\n",
        "                model = tgt_pred(l, k, a, lp, lt, th, fold, split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhKOnLlqlOQY"
      },
      "outputs": [],
      "source": [
        "# @params: \n",
        "# trust threshold (0.7) t  ***\n",
        "# mixing factor alpha (0.5)   ***\n",
        "# lambdaU,lambdaV,lambdaT (0.02, 0.02, 0.1)\n",
        "# no of iterations (100) +++\n",
        "# no of latent features K (10) ***\n",
        "# train test split  \n",
        "# learning rate\n",
        "\n",
        "# metrics\n",
        "# MAE\n",
        "# RMSE\n",
        "\n",
        "# benchmark\n",
        "# yakchi pacis\n",
        "# tobias umap\n",
        "# p2mf cdrup\n",
        "\n",
        "# variations\n",
        "# numerical personality values (0,1)\n",
        "# binary personality values [0 and 1]\n",
        "# with demography dbscan clustering\n",
        "  # ensemble through average voting\n",
        "  # ensemble through plurality voting \n",
        "# without clustering"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-q5JrmZxLhf-",
        "S2oEa-sToZoU",
        "Aq9ugeI8SDnS"
      ],
      "provenance": [],
      "mount_file_id": "18WIFTyJ5EszbWGH83695jVNsg31yf5XE",
      "authorship_tag": "ABX9TyO4uG1P7PjBT5hgNABluKkV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}