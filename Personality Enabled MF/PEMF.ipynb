{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomdeepAcharyya/Recommender-Systems/blob/main/Personality%20Enabled%20MF/PEMF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08IHqa4pgM8h"
      },
      "outputs": [],
      "source": [
        "# PEMF MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQGxLXcXklX6"
      },
      "outputs": [],
      "source": [
        "# importing packages\n",
        "import numpy as np\n",
        "import numba as nb\n",
        "import scipy.sparse as sp\n",
        "import time\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import scipy.spatial as spt\n",
        "import statistics\n",
        "import math\n",
        "import json\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from random import sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnPMs77gKxJC"
      },
      "outputs": [],
      "source": [
        "# importing packages\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY,p.OPT.MENTION,p.OPT.HASHTAG, p.OPT.ESCAPE_CHAR, p.OPT.RESERVED)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from textblob import Word\n",
        "import re\n",
        "punctuation = re.compile(r'[-.?&!,:;()|0-9]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S7t-idOyXui"
      },
      "outputs": [],
      "source": [
        "def pemf(R,user_list,item_list,S,n,m,K,alpha,lambdaU,lambdaI,lambdaS,R_test,user_list_test,item_list_test,S_test):\n",
        "    def sigmoid(z):\n",
        "        return 1 + ((5.0 - 1.0) / (1+np.exp(-z)))\n",
        "    def dsigmoid(z):\n",
        "        return (5.0 - 1.0) * np.exp(-z)/np.power((1+np.exp(-z)),2)\n",
        "    def rmse(U,I,R):\n",
        "        keylist = []\n",
        "        dok_keys = np.array(R.todok().keys()).T.reshape([1,1])[0][0]\n",
        "        for k in dok_keys:\n",
        "          row = []\n",
        "          for l in k:\n",
        "            row.append(l)\n",
        "          keylist.append(row) \n",
        "        keylist = np.array(keylist)\n",
        "        utl = keylist[:, 0]\n",
        "        itl = keylist[:, 1]\n",
        "        error = (get_csrmat(sigmoid(U.dot(I.T)),utl,itl)-R).power(2).sum()/R.nnz\n",
        "        error = np.sqrt((get_csrmat(sigmoid(U.dot(I.T)),utl,itl)-R).power(2)).sum()/np.sqrt(R.nnz)\n",
        "        return np.sqrt(error)\n",
        "    def mae(U,I,R):\n",
        "        keylist = []\n",
        "        dok_keys = np.array(R.todok().keys()).T.reshape([1,1])[0][0]\n",
        "        for k in dok_keys:\n",
        "          row = []\n",
        "          for l in k:\n",
        "            row.append(l)\n",
        "          keylist.append(row) \n",
        "        keylist = np.array(keylist)\n",
        "        utl = keylist[:, 0]\n",
        "        itl = keylist[:, 1]\n",
        "        error = abs(get_csrmat(sigmoid(U.dot(I.T)),utl,itl)-R).sum()/R.nnz \n",
        "        return error\n",
        "    def get_csrmat(mat,user_list,item_list):\n",
        "        indx = user_list*mat.shape[1]+item_list\n",
        "        return sp.csr_matrix((np.take(np.array(mat),indx),(user_list,item_list)),shape=(mat.shape[0],mat.shape[1]))\n",
        "    def costL(U,I,S,user_list,item_list):\n",
        "        tmp = U.dot(I.T)\n",
        "        Rx = get_csrmat(sigmoid(tmp),user_list,item_list)\n",
        "        cost = 0.5*((R - Rx).power(2)).sum()+0.5*lambdaU*np.linalg.norm(U)**2+0.5*lambdaI*np.linalg.norm(I)**2\n",
        "        cost += 0.5*lambdaS*np.power(U-S.dot(U),2).sum()\n",
        "        return cost\n",
        "    def gradient(U,I,S,user_list,item_list):\n",
        "        dU = np.zeros(U.shape)\n",
        "        dI = np.zeros(I.shape)\n",
        "        dU = lambdaU*U\n",
        "        tmp = U.dot(I.T)\n",
        "        Rv = get_csrmat(dsigmoid(tmp),user_list,item_list)\n",
        "        Rx = get_csrmat(sigmoid(tmp),user_list,item_list)\n",
        "        dU += Rv.multiply((Rx-R)).dot(I)\n",
        "        dU += lambdaS*(U-S.dot(U))-lambdaS*S.T.dot((U-S.dot(U)))\n",
        "        dI = lambdaI*I\n",
        "        dI += (Rv.multiply((Rx-R))).T.dot(U)\n",
        "        if np.max(dU)>1:\n",
        "            dU = dU/np.max(dU)\n",
        "        if np.max(dI)>1:\n",
        "            dI = dI/np.max(dI)\n",
        "        return dU,dI\n",
        "\n",
        "\n",
        "    def train(U,I,S,user_list,item_list):\n",
        "        res=[]\n",
        "        steps=10\n",
        "        learning_rate = alpha\n",
        "        pregradU = 0\n",
        "        pregradI = 0\n",
        "        tol=1e-6\n",
        "        momentum = 0.9\n",
        "        stage = max(steps/100 , 1)\n",
        "        for step in range(steps):\n",
        "            start = time.time()\n",
        "            dU,dI = gradient(U,I,S,user_list,item_list)\n",
        "            dU = dU + momentum*pregradU\n",
        "            dI = dI + momentum*pregradI\n",
        "            pregradU = dU\n",
        "            pregradI = dI\n",
        "            if not step%stage and rate>0.001:\n",
        "                rate = 0.95*rate\n",
        "            U -= rate * dU\n",
        "            I -= rate * dI\n",
        "            e = costL(U,I,S,user_list,item_list) / (len(U) * len(I))\n",
        "            res.append(e)\n",
        "            if not step%stage:\n",
        "                #print(\"step\",step,\"error\", e, \"time\", time.time() - start)\n",
        "                #print(\"RMSE\",rmse(U,V,R), \"MAE\",mae(U,V,R))\n",
        "                e1 = e\n",
        "            if step>150:# or abs(sum(res[-3:])-sum(res[-13:-10]))<tol:\n",
        "                #print(\"====================\")\n",
        "                #print(\"stop in %d step\"%(step))\n",
        "                #print(\"error is \",e)\n",
        "                #print(\"====================\")\n",
        "                break\n",
        "        return U, I\n",
        "\n",
        "\n",
        "    U = np.random.normal(0,0.01,size=(R.shape[0],k))\n",
        "    I = np.random.normal(0,0.01,size=(R.shape[1],k))\n",
        "    Utest = np.random.normal(0,0.01,size=(R_test.shape[0],k))\n",
        "    Itest = np.random.normal(0,0.01,size=(R_test.shape[1],k))\n",
        "    start = time.time()\n",
        "    U,I = train(U,I,S,user_list,item_list)\n",
        "\n",
        "    #print(\"=================RESULT=======================\")\n",
        "    #print('K:%d,lambdaU:%s, lambdaI:%s,lambdaS:%s' \\\n",
        "    #        %(K,lambdaU,lambdaI,lambdaS))\n",
        "    #print(\"mae\",mae(U,V,R_test))\n",
        "    #print(\"time\",time.time() - start)\n",
        "    #print(\"========================================\")\n",
        "    #print(\"mae recal\", mean_absolute_error((U@V.T).toarray(), R_test))\n",
        "\n",
        "    print(\"rmse \",rmse(Utest,Itest,R_test), \"mae \",mae(Utest,Itest,R_test), \"time\",time.time() - start)\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_distance(A,A1):\n",
        "  similarity = np.dot(A, A1.T)\n",
        "  square_mag = np.diag(similarity)\n",
        "  inv_square_mag = 1 / square_mag\n",
        "  inv_square_mag[np.isinf(inv_square_mag)] = 0\n",
        "  inv_mag = np.sqrt(inv_square_mag)\n",
        "  cosine = similarity * inv_mag\n",
        "  cosine = cosine.T * inv_mag\n",
        "  return cosine"
      ],
      "metadata": {
        "id": "BxVJNZOLxc-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoh5U_kVLnio"
      },
      "source": [
        "# target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data"
      ],
      "metadata": {
        "id": "S2oEa-sToZoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpFHYiuBWgC7"
      },
      "outputs": [],
      "source": [
        "# Tripadvisor review Dataset\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/tripadvisor_reviews_with_country.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  tr = pd.read_csv(infile)\n",
        "tr = tr.rename(columns={\"username\":\"userId\", \"taObject\":\"itemId\"})\n",
        "arr = tr[['open', 'cons', 'extra', 'agree', 'neuro', 'userId', 'itemId', 'rating']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoYNhrun4QC9"
      },
      "outputs": [],
      "source": [
        "# amazon magazines personality csv\n",
        "path1 = r'/content/drive/MyDrive/Per_CD_RS/magazine_output_final.csv'\n",
        "with open(path1, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  df = pd.read_csv(infile, nrows=100)\n",
        "#arr = np.array(df[['0', '1', '2', '3', '4']])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# amazon review dataset magazines csv\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/Amazon_magazine_filtered.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  az = pd.read_csv(infile)\n",
        "#az = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})"
      ],
      "metadata": {
        "id": "NrpayR6tMfBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = pd.merge(df, az, on='userId', how='inner')\n",
        "arr = t[['open', 'cons', 'extra', 'agree', 'neuro', 'userId', 'itemId', 'rating']]\n",
        "t.shape"
      ],
      "metadata": {
        "id": "WPV1t1ZcKhdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## matrix factorization"
      ],
      "metadata": {
        "id": "57DiP3NrSIqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src = tr[['userId', 'itemId', 'rating']]\n",
        "arr = pd.DataFrame(arr)\n",
        "arr['userId'] = az['userId']\n",
        "arr['itemId'] = np.array(az['itemId'])\n",
        "arr['rating'] = az['rating']"
      ],
      "metadata": {
        "id": "rfh7EBOHmCSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoEu4fwZhLL6"
      },
      "outputs": [],
      "source": [
        "# Construction of pivot matrix of ratings ru with respect to users u\n",
        "ru = arr.pivot_table(index='userId',columns='itemId',values='rating')\n",
        "ru = ru.fillna(0)\n",
        "ru_m = ru > 0\n",
        "ru_m = ru_m.replace(True, 1)\n",
        "ru_m = ru_m.replace(False, 0)\n",
        "ru = np.array(ru)\n",
        "ru_m = np.array(ru_m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8UipsLlMESh"
      },
      "outputs": [],
      "source": [
        "ru = arr.groupby(['userId', 'itemId'])['rating'].sum().unstack()\n",
        "ru = pd.DataFrame(ru)\n",
        "ru.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dividing into train and test of rating matrix ru\n",
        "\n",
        "def split_data(ru, s, k):\n",
        "  split = s\n",
        "  k = k\n",
        "  test = []\n",
        "  set1  = ru.iloc[: , :int(ru.shape[1]/split)]\n",
        "  set1 = set1.replace(np.nan, 0)\n",
        "  for i in range(k):\n",
        "    set2 = ru.iloc[: , i*int(ru.shape[1]/split):(i+1)*int(ru.shape[1]/split)]\n",
        "    set2 = set2.replace(np.nan, 0)\n",
        "    set1 = pd.concat([set1, set2], axis=1)\n",
        "    set1  = ru.iloc[: , :int(ru.shape[1]/split)]\n",
        "    set1 = set1.replace(np.nan, 0)\n",
        "    set2  = ru.iloc[: , int(ru.shape[1]/split): 2*int(ru.shape[1]/split)]\n",
        "    set2 = set2.replace(np.nan, 0)\n",
        "    xx = pd.concat([set1, set2], axis=1)\n",
        "    test.append(xx)\n",
        "  return test"
      ],
      "metadata": {
        "id": "JunMaXAV9bhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mprzddQjhRfs"
      },
      "outputs": [],
      "source": [
        "# with only simpers and simrate use this\n",
        "\n",
        "def tgt_pred(a, k, b, lp, ls, g, fold, split):\n",
        "\n",
        "    train_size = split\n",
        "    lr, k, beta, lambda_u, lambda_s, thr = a, k, b, lp, ls, g\n",
        "    df_copy = arr.copy()\n",
        "    train_set = df_copy.sample(frac=train_size).reset_index()\n",
        "    user_features_train = np.array(train_set[['open', 'cons', 'extra', 'agree', 'neuro']].fillna(0))\n",
        "    #test_set = df_copy.drop(train_set.index).reset_index()\n",
        "    test_set = pd.DataFrame(split_data(df_copy, (1-train_size)*10, fold)[0])\n",
        "    user_features_test = np.array(test_set[['open', 'cons', 'extra', 'agree', 'neuro']].fillna(0))\n",
        "\n",
        "    df = pd.DataFrame(user_features_train) \n",
        "    df.columns = ['open', 'cons', 'extra', 'agree', 'neuro']\n",
        "    df['userId'] = train_set.userId\n",
        "    df2 = df.groupby(by='userId').mean().reset_index()\n",
        "    df3 = np.array(df2[[\"open\", 'cons', 'extra', 'agree', 'neuro']])\n",
        "    \n",
        "\n",
        "    \n",
        "    ru = train_set.groupby(['userId', 'itemId'])['rating'].sum().unstack()\n",
        "    ru = pd.DataFrame(ru)\n",
        "    set1 = ru.iloc[: , :int(ru.shape[1]/(1-train_size)*10)]\n",
        "    set1 = set1.replace(np.nan, 0)\n",
        "    #for i in range(1,2):\n",
        "    # set2 = ru.iloc[: , i*int(ru.shape[1]/10):(i+1)*int(ru.shape[1]/10)]\n",
        "    #  set2 = set2.replace(np.nan, 0)\n",
        "    #  set1 = pd.concat([set1, set2], axis=1)\n",
        "    ru = set1\n",
        "    ru_m = ru > 0\n",
        "    ru_m = ru_m.replace(True, 1)\n",
        "    ru_m = ru_m.replace(False, 0)\n",
        "    ru = np.array(ru)\n",
        "    ru_m = np.array(ru_m)\n",
        "\n",
        "    R0 = ru    # rating matrix\n",
        "    max_r = 5.0    # max _rating   \n",
        "    N,M= ru.shape[0],ru.shape[1]     # # n : no of users   m : no of items \n",
        "    K = k                       #k : latent dimension        (mx10)@(10xn)\n",
        "    lambdaU,lambdaI,lambdaS= lambda_u, lambda_u, lambda_a\n",
        "    keys = []\n",
        "\n",
        "    R=sp.dok_matrix((N,M))   # create sparse matrix for user x item\n",
        "    T=sp.dok_matrix((N,N))   # create sparse matrix for trust among users user x user\n",
        "    for i in range(len(R0)):    # no of users\n",
        "        for j in range(len(R0[i])):   # no of items\n",
        "            if R0[i][j]>0:    # if rating is present   \n",
        "                keys.append([i,j])\n",
        "                R[i,j] = R0[i][j]     \n",
        "\n",
        "    dft = pd.DataFrame(user_features_test) \n",
        "    dft.columns = ['open', 'cons', 'extra', 'agree', 'neuro']\n",
        "    dft['userId'] = test_set.userId\n",
        "    df2t = dft.groupby(by='userId').mean().reset_index()\n",
        "    df3t = np.array(df2t[[\"open\", 'cons', 'extra', 'agree', 'neuro']])\n",
        "\n",
        "    rut = test_set.pivot_table(index='userId',columns='itemId',values='rating')\n",
        "    rut = test_set.groupby(['userId', 'itemId'])['rating'].sum().unstack()\n",
        "    rut = pd.DataFrame(rut)\n",
        "    rut = rut.fillna(0)\n",
        "    ru_mt = rut > 0\n",
        "    ru_mt = ru_mt.replace(True, 1)\n",
        "    ru_mt = ru_mt.replace(False, 0)\n",
        "    rut = np.array(rut)\n",
        "    ru_mt = np.array(ru_mt)\n",
        "\n",
        "    R0t = rut    # rating matrix\n",
        "    max_rt = 5.0    # max _rating   \n",
        "    Nt,Mt= rut.shape[0],rut.shape[1]     # # n : no of users   m : no of items \n",
        "    K = k                        #k : latent dimension        (mxk)@(kxn)\n",
        "    lambdaU,lambdaI,lambdaS = lambda_u, lambda_u, lambda_s\n",
        "    keyst = []\n",
        "\n",
        "    Rt = sp.dok_matrix((Nt,Mt))   # create sparse matrix for user x item\n",
        "    Tt = sp.dok_matrix((Nt,Nt))   # create sparse matrix for user x user\n",
        "    for i in range(len(R0t)):    # no of users\n",
        "        for j in range(len(R0t[i])):   # no of items\n",
        "            if R0t[i][j]>0:    # if rating is present   \n",
        "                keyst.append([i,j])\n",
        "                Rt[i,j] = R0t[i][j]      \n",
        "\n",
        "    \n",
        "    # get trust factor between users trust_uv(user x user)  and sim_uv(user x user) t_uv(user x user)\n",
        "\n",
        "\n",
        "    simpers_uv = 1 - calc_distance(df3, df3)\n",
        "    simpers_uv = np.nan_to_num(trust_uv)\n",
        "    simpers_uv_s = simpers_uv.copy()\n",
        "    simpers_uv = pd.DataFrame(simpers_uv >= thr)\n",
        "    simpers_uv = simpers_uv.replace(True, 1)\n",
        "    simpers_uv = simpers_uv.replace(False, 0)\n",
        "    simpers_uv = np.array(simpers_uv)\n",
        "\n",
        "    neighbours = []\n",
        "    for j in range(len(trust_uv)):\n",
        "      row = []\n",
        "      for m in range(len(trust_uv[j])):\n",
        "        if trust_uv[j][m] == 1:\n",
        "          row.append(m)\n",
        "      neighbours.append(row)\n",
        "    neighbours = np.array(neighbours)\n",
        "    simrate_uv = 1- calc_distance(ru, ru)\n",
        "    simrate_uv = np.nan_to_num(simrate_uv)\n",
        "    #ru_tf2 = ru_tf[0:trust_uv.shape[0]]\n",
        "    #ru_tf2 = ru_tf[:,:len(trust_uv)]\n",
        "    s_uv = np.add(beta * simpers_uv_s, (1-beta)* simrate_uv)\n",
        "    #s_uv = np.multiply(np.add(beta * trust_uv_s, (1-beta)* sim_uv), ru_tf)\n",
        "\n",
        "    max_value = max(s_uv.flatten())\n",
        "    min_value = min(s_uv.flatten())\n",
        "    S_df = pd.DataFrame(s_uv)\n",
        "    S_df = S_df >=  np.subtract(S_df, min_value) /(max_value - min_value)   # thr is threshold value\n",
        "    S_df = S_df.replace(True, 1)\n",
        "    S_df = S_df.replace(False, 0)\n",
        "    S_df = np.array(S_df)\n",
        "    S = sp.csr_matrix(S_df)   # fill up trust matrix from user realtionships   *******\n",
        "\n",
        "\n",
        "    simpers_uv_t = 1 - calc_distance(df3t, df3t)\n",
        "    simpers_uv_t = np.nan_to_num(simpers_uv_t)\n",
        "    simpers_uv_st = simpers_uv_t.copy()\n",
        "    simpers_uv_t = pd.DataFrame(simpers_uv_t >= thr)\n",
        "    simpers_uv_t = simpers_uv_t.replace(True, 1)\n",
        "    simpers_uv_t = simpers_uv_t.replace(False, 0)\n",
        "    simpers_uv_t = np.array(simpers_uv_t)\n",
        "\n",
        "    neighbours_t = []\n",
        "    for j in range(len(simpers_uv_t)):\n",
        "      row = []\n",
        "      for m in range(len(trust_uv_t[j])):\n",
        "        if trust_uv_t[j][m] == 1:\n",
        "          row.append(m)\n",
        "      neighbours_t.append(row)\n",
        "    neighbours_t = np.array(neighbours_t)\n",
        "    simrate_uv_t = 1- calc_distance(rut, rut)\n",
        "    simrate_uv_t = np.nan_to_num(simrate_uv_t)\n",
        "    #ru_tf2 = ru_tf[0:trust_uv.shape[0]]\n",
        "    #ru_tf2 = ru_tf[:,:len(trust_uv)]\n",
        "    s_uv_t = np.add(beta * simpers_uv_st, (1-beta)* simrate_uv_t)\n",
        "    #t_uv = np.multiply(np.add(beta * trust_uv_s, (1-beta)* sim_uv), ru_tf)\n",
        "\n",
        "    max_value = max(s_uv_t.flatten())\n",
        "    min_value = min(s_uv_t.flatten())\n",
        "    S_df_t = pd.DataFrame(s_uv_t)\n",
        "    S_df_t = S_df_t >=  np.subtract(S_df_t, min_value) /(max_value - min_value)   # thr is threshold value\n",
        "    S_df_t = S_df_t.replace(True, 1)\n",
        "    S_df_t = S_df_t.replace(False, 0)\n",
        "    S_df_t = np.array(S_df_t)\n",
        "    St = sp.csr_matrix(S_df_t)   # fill up trust matrix from user realtionships   *******\n",
        "\n",
        "\n",
        "    \n",
        "    keys = np.array(keys)        # get the keys\n",
        "    keyst = np.array(keyst)\n",
        "    R,S = R.tocsr(),S.tocsr()\n",
        "    Rt,St = Rt.tocsr(), St.tocsr()\n",
        "    pemf(R,keys[:, 0],keys[:, 1],T,N,M,K,lambdaU,lambdaI,lambdaS,Rt,keyst[:, 0],keyst[:, 1],St)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "WqL-UmnioR4R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQSWNzk40RTL"
      },
      "outputs": [],
      "source": [
        "alpha = [0.0001]\n",
        "K = [40]\n",
        "beta = [0.5]\n",
        "lambda_u = [0.02]\n",
        "lambda_s = [0.5]\n",
        "gamma = [0.7]\n",
        "fold = 1\n",
        "split = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng3F_bKdaLDn"
      },
      "outputs": [],
      "source": [
        "a, k, b, lp, ls, g = 0.0001, 20, 0.5, 0.03, 0.8, 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW8lXBJal7Se",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648dcd9d-973f-4ce3-a6fb-21e102f6d295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0001 40 0.5 0.02 0.5 0.7\n",
            "rmse  7.51462790680989 mae  2.8779589644793466 time 0.06507492065429688\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  for i in range(fold):\n",
        "    for a in alpha:\n",
        "      for k in K:\n",
        "        for b in beta:\n",
        "          for lp in lambda_u:\n",
        "            for ls in lambda_s:\n",
        "              for g in gamma:\n",
        "                print(a, k, b, lp, ls, g)\n",
        "                model = tgt_pred(a, k, b, lp, ls, g, fold, split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhKOnLlqlOQY"
      },
      "outputs": [],
      "source": [
        "# @params: \n",
        "# trust threshold (0.7) g: gamma ***\n",
        "# mixing factor (0.5) b: beta  ***\n",
        "# lambdaU,lambdaI,lambdaS (0.02, 0.02, 0.1)\n",
        "# no of iterations (100) +++\n",
        "# no of latent features K (10) ***\n",
        "# train test split  \n",
        "# learning rate a: alpha\n",
        "\n",
        "# metrics\n",
        "# MAE\n",
        "# RMSE\n",
        "\n",
        "\n",
        "# variations\n",
        "# numerical personality values (0,1)\n",
        "# binary personality values [0 and 1]\n",
        "# with demography dbscan clustering\n",
        "  # ensemble through average voting\n",
        "  # ensemble through plurality voting \n",
        "# without clustering"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18WIFTyJ5EszbWGH83695jVNsg31yf5XE",
      "authorship_tag": "ABX9TyNrUJOPGezqfOXX216zq+6x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}