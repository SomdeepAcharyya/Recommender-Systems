{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomdeepAcharyya/Recommender-Systems/blob/main/Social_Trust_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Social Trust Ensemble \n",
        "# Learning to Recommend with Social Trust Ensemble\n",
        "# Hao Ma, Irwin King and Michael R. Lyu"
      ],
      "metadata": {
        "id": "ZPZk_hmDjdP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-4gfOJNrV_k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import time\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "import scipy.spatial as spt\n",
        "import statistics\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9Fydf7krte7"
      },
      "outputs": [],
      "source": [
        "def RSTE(R,S,N,M,K,lambdaU,lambdaV,lambdaT,R_test,ul,il):\n",
        "    def sigmoid(z):\n",
        "        return 1.0 / (1+np.exp(-z))\n",
        "    def dsigmoid(z):\n",
        "        return np.exp(-z)/np.power((1+np.exp(-z)),2)\n",
        "    def rmse(U,V,R):\n",
        "        keylist = []\n",
        "        dok_keys = np.array(R.todok().keys()).T.reshape([1,1])[0][0]\n",
        "        for k in dok_keys:\n",
        "          row = []\n",
        "          for l in k:\n",
        "            row.append(l)\n",
        "          keylist.append(row) \n",
        "        keylist = np.array(keylist)\n",
        "        utl = keylist[:, 0]\n",
        "        itl = keylist[:, 1]\n",
        "        error = (get_csrmat(sigmoid(U.dot(V.T)),utl,itl)-R).power(2).sum()/R.nnz\n",
        "        return 5*np.sqrt(error)\n",
        "    def mae(U,V,R):\n",
        "        keylist = []\n",
        "        dok_keys = np.array(R.todok().keys()).T.reshape([1,1])[0][0]\n",
        "        for k in dok_keys:\n",
        "          row = []\n",
        "          for l in k:\n",
        "            row.append(l)\n",
        "          keylist.append(row) \n",
        "        keylist = np.array(keylist)\n",
        "        utl = keylist[:, 0]\n",
        "        itl = keylist[:, 1]\n",
        "        error = abs(get_csrmat(sigmoid(U.dot(V.T)),utl,itl)-R).sum()/R.nnz\n",
        "        return error\n",
        "    def get_csrmat(mat,ul,il):\n",
        "        indx = ul*mat.shape[1]+il\n",
        "        return sp.csr_matrix((np.take(np.array(mat),indx),(ul,il)),shape=(N,M))\n",
        "    def costL(U,V):\n",
        "        tmp = lambdaT*U.dot(V.T)+(1-lambdaT)*S.dot((U.dot(V.T)))\n",
        "        Rx = get_csrmat(sigmoid(tmp),ul,il)\n",
        "        cost = 0.5*((R - Rx).power(2)).sum()+0.5*lambdaU*np.linalg.norm(U)**2+0.5*lambdaV*np.linalg.norm(V)**2\n",
        "        return cost\n",
        "    def gradient(U,V):\n",
        "        dU = lambdaU*U\n",
        "        tmp = lambdaT*U.dot(V.T)+(1-lambdaT)*S.dot((U.dot(V.T)))\n",
        "        Rv = get_csrmat(dsigmoid(tmp),ul,il)\n",
        "        Rx = get_csrmat(sigmoid(tmp),ul,il)\n",
        "        matx = Rv.multiply((Rx-R)).dot(V)\n",
        "        dU += lambdaT*matx\n",
        "        dU += (1-lambdaT)*(S.T).dot(matx)\n",
        "        dV = lambdaV*V\n",
        "        dV += (Rv.multiply((Rx-R))).T.dot(lambdaT*U+(1-lambdaT)*S.dot(U))\n",
        "        # print dU,dV\n",
        "        if np.max(dU)>1:\n",
        "            dU = dU/np.max(dU)\n",
        "        if np.max(dV)>1:\n",
        "            dV = dV/np.max(dV)\n",
        "        return dU,dV\n",
        "\n",
        "    def train(U,V):\n",
        "        res=[]\n",
        "        steps=15\n",
        "        rate = 1e-4\n",
        "        pregradU = 0\n",
        "        pregradV = 0\n",
        "        tol=1e-3\n",
        "        momentum = 0.9\n",
        "        stage = max(steps/100 , 1)\n",
        "        for step in range(steps):\n",
        "            start = time.time()\n",
        "            dU,dV = gradient(U,V)\n",
        "            dU = dU + momentum*pregradU\n",
        "            dV = dV + momentum*pregradV\n",
        "            pregradU = dU\n",
        "            pregradV = dV\n",
        "            if not step%stage and rate>0.0001:\n",
        "                rate = 0.95*rate\n",
        "            U -= rate * dU\n",
        "            V -= rate * dV\n",
        "            e = costL(U,V) / (len(U) * len(V))\n",
        "            res.append(e)\n",
        "            if not step%(stage*5):\n",
        "                print(step,\"mae\", e,\"rmse\", e*34.6410161514, time.time()-start)\n",
        "            if step>150 or abs(sum(res[-3:])-sum(res[-13:-10]))<tol:\n",
        "                print(\"====================\")\n",
        "                print(\"stop in %d step\"%(step))\n",
        "                print(\"error is \",e)\n",
        "                print(\"====================\")\n",
        "                break\n",
        "        return U, V\n",
        "    U = np.random.normal(0,0.01,size=(N,K))\n",
        "    V = np.random.normal(0,0.01,size=(M,K))\n",
        "    U,V = train(U,V)\n",
        "    print(\"=================RESULT=======================\")\n",
        "    print('K:%d,lambdaU:%s, lambdaV:%s,lambdaT:%s' \\\n",
        "            %(K,lambdaU,lambdaV,lambdaT))\n",
        "    print(\"rmse\",rmse(U,V,R_test))\n",
        "    print(\"mae\",mae(U,V,R_test))\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rja_cLdHMXbw"
      },
      "source": [
        "# example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI50JeWXr1RG"
      },
      "outputs": [],
      "source": [
        "def t_yelp(limitu,limiti):\n",
        "    #data from: http://www.trustlet.org/wiki/Epinions_datasets\n",
        "    def getdata():\n",
        "        N,M = limitu,limiti\n",
        "        max_r = 5.0\n",
        "        cNum = 8\n",
        "        T=sp.dok_matrix((N,N))\n",
        "        print('get T')\n",
        "        for line in open('./yelp_data/users.txt','r'):\n",
        "            u = int(line.split(':')[0])\n",
        "            uf = line.split(':')[1][1:-1].split(',')\n",
        "            if len(uf)>1:\n",
        "                for x in line.split(':')[1][1:-1].split(',')[:-1]:\n",
        "                    v = int(x)\n",
        "                    if u<limitu and v<limitu:\n",
        "                        T[u,v] = 1.0\n",
        "        T = T.tocsr()\n",
        "        print('get R_test')\n",
        "        utl,itl,rtl = [],[],[]\n",
        "        for line in open('./yelp_data/ratings-test.txt','r'):\n",
        "            u,i,r = [int(x) for x in line.split('::')[:3]]\n",
        "            if u<limitu and i<limiti:\n",
        "                utl.append(u)\n",
        "                itl.append(i)\n",
        "                rtl.append(r/5.0)\n",
        "        utl,itl = np.array(utl),np.array(itl)\n",
        "        R_test = sp.csr_matrix((rtl,(utl,itl)),shape=(N,M))\n",
        "        print('get R')\n",
        "        ul,il,rl = [],[],[]\n",
        "        for line in open('./yelp_data/ratings-train.txt','r'):\n",
        "            u,i,r = [int(x) for x in line.split('::')[:3]]\n",
        "            if u<limitu and i<limiti:\n",
        "                ul.append(u)\n",
        "                il.append(i)\n",
        "                rl.append(r/5.0)\n",
        "        ul,il = np.array(ul),np.array(il)\n",
        "        R = sp.csr_matrix((rl,(ul,il)),shape=(N,M))\n",
        "        # print \"get Circle\"\n",
        "        # C = [[] for i in range(cNum)]\n",
        "        # for line in open('./yelp_data/items-class.txt','r'):\n",
        "        #     i,ci = [int(x) for x in line.split(' ')]\n",
        "        #     if i<limit:\n",
        "        #         C[ci].append(i)\n",
        "        return R,T,N,M,R_test,ul,il\n",
        "    R,T,N,M,R_test,ul,il = getdata()\n",
        "    lambdaU,lambdaV,lambdaT,K = 0.01, 0.01, 0.01, 5\n",
        "    RSTE(R,T,N,M,K,lambdaU,lambdaV,lambdaT,R_test,ul,il)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qE_pDg0tpDr"
      },
      "outputs": [],
      "source": [
        "def t_toy_correct():\n",
        "    R0 = [\n",
        "         [5,3,0,1],\n",
        "         [4,0,0,1],\n",
        "         [1,1,0,5],\n",
        "         [1,0,0,4],\n",
        "         [0,1,5,4],\n",
        "        ]     # rating matrix\n",
        "    max_r = 5.0    # max _rating\n",
        "    T0 = [[3,2],[1,3,4],[2],[1,5],[3]]     # trust relationship\n",
        "    N,M,K=5,4,4     # n : no of users   m : no of items k : latent dimension        (5x4)@(4x4)\n",
        "    lambdaU,lambdaV,lambdaT=0.02, 0.02, 0.1\n",
        "    keys = []\n",
        "\n",
        "    R=sp.dok_matrix((N,M))   # create sparse matrix for user x item\n",
        "    T=sp.dok_matrix((N,N))   # create sparse matrix for trust among users user x user\n",
        "    for i in range(len(R0)):    # no of users\n",
        "        for j in range(len(R0[0])):   # no of items\n",
        "            if R0[i][j]>0:    # if rating is present   \n",
        "                keys.append([i,j])\n",
        "                R[i,j] = 1.0 * R0[i][j] / max_r      # normalise the rating matrix     R is the new normalised rating matrix which will be used *******\n",
        "    for i in range(len(T0)):    # no of users\n",
        "        for j in T0[i]:         # no of trusted users of user i\n",
        "            T[i, j-1]=1.0        # fill up trust matrix from user realtionships   *******\n",
        "    keys = np.array(keys)        # get the keys\n",
        "    R,T = R.tocsr(),T.tocsr()\n",
        "    RSTE(R,T,N,M,K,lambdaU,lambdaV,lambdaT,R,keys[:, 0],keys[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP7upebctxaH",
        "outputId": "d797374f-eba3-430d-810f-dcbba1394463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0.7850324398340742 0.008330821990966797\n",
            "50 0.7850309156871187 0.0016372203826904297\n",
            "100 0.785029102448922 0.002238750457763672\n",
            "====================\n",
            "stop in 101 step\n",
            "error is  0.7850290661750954\n",
            "====================\n",
            "=================RESULT=======================\n",
            "K:4,lambdaU:0.02, lambdaV:0.02,lambdaT:0.1\n",
            "rmse 1.7375782356560296\n",
            "mae 0.3307669730422547\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "#   t_epinion()\n",
        "   t_toy_correct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ_WhsdmMScT"
      },
      "source": [
        "# target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL8YH1L_whDg"
      },
      "outputs": [],
      "source": [
        "# amazon review dataset magazines csv\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/pers_fashion_filtered.csv'\n",
        "\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  df = pd.read_csv(infile)\n",
        "arr = np.array(df[['0', '1', '2', '3', '4']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJhxQDBQw3jO"
      },
      "outputs": [],
      "source": [
        "# amazon review dataset magazines csv\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/Amazon_fashion_filtered.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  az = pd.read_csv(infile)\n",
        "az = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tripadvisor review Dataset\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/tripadvisor_reviews_with_country.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  tr = pd.read_csv(infile)\n",
        "tr = tr.rename(columns={\"username\":\"userId\", \"taObject\":\"itemId\"})\n",
        "arr = tr[['open', 'cons', 'extra', 'agree', 'neuro', 'userId', 'itemId', 'rating']]"
      ],
      "metadata": {
        "id": "h5Mtgegk7L_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6YS0XtqoeXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "761ebffd-445a-43d9-83c4-3a2193ac8f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7841, 5) (7891, 6)\n",
            "(7841, 5) (7841, 6)\n"
          ]
        }
      ],
      "source": [
        "print(arr.shape, az.shape)\n",
        "az = az[0:len(arr)]\n",
        "print(arr.shape, az.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = pd.DataFrame(arr)"
      ],
      "metadata": {
        "id": "gyYUHD9HPQMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zOUfZyBw7cW"
      },
      "outputs": [],
      "source": [
        "arr['userId'] = az['userId']\n",
        "arr['itemId'] = az['itemId']\n",
        "arr['rating'] = az['rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikd5WS6Aw-8e"
      },
      "outputs": [],
      "source": [
        "arr.columns = ['open', 'cons', 'extra', 'agree', 'neuro', 'userId', 'itemId', 'rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5kjhO7FxCN7"
      },
      "outputs": [],
      "source": [
        "# rating matrix wrt user u\n",
        "ru = arr.pivot_table(index='userId',columns='itemId',values='rating')\n",
        "ru = ru.fillna(0)\n",
        "ru_m = ru > 0\n",
        "ru_m = ru_m.replace(True, 1)\n",
        "ru_m = ru_m.replace(False, 0)\n",
        "ru = np.array(ru)\n",
        "ru_m = np.array(ru_m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wl5wsNm0NxF"
      },
      "outputs": [],
      "source": [
        "# with only trust and rating test split\n",
        "\n",
        "def tgt_pred(lr, lambda_p, lambda_t, t, k):\n",
        "\n",
        "    lr, lambda_p, lambda_t, t, k = lr, lambda_p, lambda_t, t, k\n",
        "    train_size = 0.8\n",
        "    df_copy = arr.copy()\n",
        "    train_set = df_copy.sample(frac=train_size).reset_index()\n",
        "    user_features_train = np.array(train_set[['open', 'cons', 'extra', 'agree', 'neuro']].fillna(0))\n",
        "    test_set = df_copy.drop(train_set.index).reset_index()\n",
        "    user_features_test = np.array(test_set[['open', 'cons', 'extra', 'agree', 'neuro']].fillna(0))\n",
        "\n",
        "    df = pd.DataFrame(user_features_train) \n",
        "    df.columns = ['open', 'cons', 'extra', 'agree', 'neuro']\n",
        "    df['userId'] = train_set.userId\n",
        "    df2 = df.groupby(by='userId').mean().reset_index()\n",
        "    df3 = np.array(df2[[\"open\", 'cons', 'extra', 'agree', 'neuro']])\n",
        "\n",
        "    #ru = train_set.pivot_table(index='userId',columns='itemId',values='rating')\n",
        "    #ru = ru.fillna(0)\n",
        "    ru = train_set.groupby(['userId', 'itemId'])['rating'].sum().unstack()\n",
        "    ru = pd.DataFrame(ru)\n",
        "    ru = ru.replace(np.nan, 0)\n",
        "    ru_m = ru > 0\n",
        "    ru_m = ru_m.replace(True, 1)\n",
        "    ru_m = ru_m.replace(False, 0)\n",
        "    ru = np.array(ru)\n",
        "    ru_m = np.array(ru_m)\n",
        "\n",
        "    R0 = ru  # rating matrix\n",
        "    max_r = 5.0    # max _rating   \n",
        "    N,M= ru.shape[0],ru.shape[1]     # # n : no of users   m : no of items \n",
        "    K = k                         #k : latent dimension        (mx10)@(10xn)\n",
        "    lambdaU,lambdaV,lambdaT=lambda_p, lambda_p, lambda_t\n",
        "    keys = []\n",
        "\n",
        "    R=sp.dok_matrix((N,M))   # create sparse matrix for user x item\n",
        "    T=sp.dok_matrix((N,N))   # create sparse matrix for trust among users user x user\n",
        "    for i in range(len(R0)):    # no of users\n",
        "        for j in range(len(R0[i])):   # no of items\n",
        "            if R0[i][j]>0:    # if rating is present   \n",
        "                keys.append([i,j])\n",
        "                R[i,j] = R0[i][j]      # normalise the rating matrix     R is the new normalised rating matrix which will be used *******\n",
        "\n",
        "    dft = pd.DataFrame(user_features_test) \n",
        "    dft.columns = ['open', 'cons', 'extra', 'agree', 'neuro']\n",
        "    dft['userId'] = test_set.userId\n",
        "    df2t = dft.groupby(by='userId').mean().reset_index()\n",
        "    df3t = np.array(df2t[[\"open\", 'cons', 'extra', 'agree', 'neuro']])\n",
        "\n",
        "    #rut = test_set.pivot_table(index='userId',columns='itemId',values='rating')\n",
        "    #rut = rut.fillna(0)\n",
        "    #ru_mt = rut > 0\n",
        "    #ru_mt = ru_mt.replace(True, 1)\n",
        "    #ru_mt = ru_mt.replace(False, 0)\n",
        "    #rut = np.array(rut)\n",
        "    #ru_mt = np.array(ru_mt)\n",
        "\n",
        "    #R0t = rut    # rating matrix\n",
        "    #max_rt = 5.0    # max _rating   \n",
        "    #Nt,Mt= rut.shape[0],rut.shape[1]     # # n : no of users   m : no of items \n",
        "    K = k                        #k : latent dimension        (mx10)@(10xn)\n",
        "    lambdaU,lambdaV,lambdaT=lambda_p, lambda_p, lambda_t\n",
        "    keyst = []\n",
        "\n",
        "    #Rt=sp.dok_matrix((Nt,Mt))   # create sparse matrix for user x item\n",
        "    #for i in range(len(R0t)):    # no of users\n",
        "    #    for j in range(len(R0t[i])):   # no of items\n",
        "    #        if R0t[i][j]>0:    # if rating is present   \n",
        "    #            keyst.append([i,j])\n",
        "    #            Rt[i,j] = R0t[i][j]      # normalise the rating matrix     R is the new normalised rating matrix which will be used *******\n",
        "\n",
        "\n",
        "    \n",
        "    # get trust factor between users trust_uv(user x user)  and sim_uv(user x user) t_uv(user x user)\n",
        "\n",
        "\n",
        "    trust_uv = 1 - spt.distance.cdist(df3, df3, 'cosine')\n",
        "    trust_uv = np.nan_to_num(trust_uv)\n",
        "    trust_uv_s = trust_uv.copy()\n",
        "    trust_uv = pd.DataFrame(trust_uv > t)\n",
        "    trust_uv = trust_uv.replace(True, 1)\n",
        "    trust_uv = trust_uv.replace(False, 0)\n",
        "    trust_uv = np.array(trust_uv)\n",
        "    tknn = []\n",
        "    for j in range(len(trust_uv)):\n",
        "      row = []\n",
        "      for x in range(len(trust_uv[j])):\n",
        "        if trust_uv[j][x] == 1:\n",
        "          row.append(x)\n",
        "      tknn.append(row)\n",
        "    tknn = np.array(tknn)\n",
        "    #sim_uv = 1 - spt.distance.cdist(ru_m, ru_m, 'cosine')\n",
        "    #sim_uv = np.nan_to_num(sim_uv)\n",
        "    #t_uv = (np.add(trust_uv, sim_uv)/2)\n",
        "    t_uv = trust_uv\n",
        "    print(t_uv.shape)\n",
        "\n",
        "    max_value = max(t_uv.flatten())\n",
        "    T_df = pd.DataFrame(t_uv)\n",
        "    T_df = T_df >= t * max_value\n",
        "    T_df = T_df.replace(True, 1)\n",
        "    T_df = T_df.replace(False, 0)\n",
        "    T_df = np.array(T_df)\n",
        "    T = sp.csr_matrix(T_df)\n",
        "           # fill up trust matrix from user realtionships   *******\n",
        "    keys = np.array(keys)        # get the keys\n",
        "    R,T = R.tocsr(),T.tocsr()\n",
        "    print(\"entring rste\")\n",
        "    print(lr, lambda_p, lambda_t, t, k)\n",
        "    RSTE(R,T,N,M,K,lambdaU,lambdaV,lambdaT,R,keys[:, 0],keys[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99KvDLJ8xc5C"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "#   t_epinion()\n",
        "#   t_toy_correct()\n",
        "    tgt_pred()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvdK_mMQM1id"
      },
      "outputs": [],
      "source": [
        "# with tf idf of rating\n",
        "\n",
        "def tgt_pred_log():\n",
        "    R0 = ru    # rating matrix\n",
        "    max_r = 5.0    # max _rating\n",
        "    T0 = [[3,2],[1,3,4],[2],[1,5],[3]]     # trust relationship\n",
        "    N,M= ru.shape[0],ru.shape[1]     # # n : no of users   m : no of items \n",
        "    K = 10                         #k : latent dimension        (mx10)@(10xn)\n",
        "    lambdaU,lambdaV,lambdaT=0.02, 0.02, 0.1\n",
        "    keys = []\n",
        "\n",
        "    R=sp.dok_matrix((N,M))   # create sparse matrix for user x item\n",
        "    T=sp.dok_matrix((N,N))   # create sparse matrix for trust among users user x user\n",
        "    for i in range(len(R0)):    # no of users\n",
        "        for j in range(len(R0[i])):   # no of items\n",
        "            if R0[i][j]>0:    # if rating is present   \n",
        "                keys.append([i,j])\n",
        "                R[i,j] = R0[i][j]      # normalise the rating matrix     R is the new normalised rating matrix which will be used *******\n",
        "\n",
        "    \n",
        "    # get trust factor between users trust_uv(user x user)  and sim_uv(user x user) t_uv(user x user)\n",
        "    df = pd.DataFrame(tgt_pers) \n",
        "    df.columns = ['open', 'cons', 'extra', 'agree', 'neuro']\n",
        "    df['userId'] = tgt.userId\n",
        "    df2 = df.groupby(by='userId').mean().reset_index()\n",
        "    df2 = np.array(df2[[\"open\", 'cons', 'extra', 'agree', 'neuro']])\n",
        "    trust_uv = 1 - spt.distance.cdist(df2, df2, 'cosine')\n",
        "    trust_uv = np.nan_to_num(trust_uv)\n",
        "    trust_uv_s = trust_uv.copy()\n",
        "    trust_uv = pd.DataFrame(trust_uv > 0.7)\n",
        "    trust_uv = trust_uv.replace(True, 1)\n",
        "    trust_uv = trust_uv.replace(False, 0)\n",
        "    trust_uv = np.array(trust_uv)\n",
        "    tknn = []\n",
        "    for j in range(len(trust_uv)):\n",
        "      row = []\n",
        "      for k in range(len(trust_uv[j])):\n",
        "        if trust_uv[j][k] == 1:\n",
        "          row.append(k)\n",
        "      tknn.append(row)\n",
        "    tknn = np.array(tknn)\n",
        "    sim_uv = 1 - spt.distance.cdist(ru_m, ru_m, 'cosine')\n",
        "    sim_uv = np.nan_to_num(sim_uv)\n",
        "    t_uv = (np.add(trust_uv, sim_uv)/2)\n",
        "    #t_uv = np.multiply(t_uv, ru_tf)\n",
        "    print(t_uv.shape)\n",
        "\n",
        "\n",
        "    max_value = max(t_uv.flatten())\n",
        "    T_df = pd.DataFrame(t_uv)\n",
        "    T_df = T_df >= 0.7 * max_value\n",
        "    T_df = T_df.replace(True, 1)\n",
        "    T_df = T_df.replace(False, 0)\n",
        "    T_df = np.array(T_df)\n",
        "    T = sp.csr_matrix(T_df)      # fill up trust matrix from user realtionships   *******\n",
        "    keys = np.array(keys)        # get the keys\n",
        "    R,T = R.tocsr(),T.tocsr()\n",
        "    print(\"entering RSTE\")\n",
        "    RSTE(R,T,N,M,K,lambdaU,lambdaV,lambdaT,R,keys[:, 0],keys[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFlQH-I2s6t7"
      },
      "outputs": [],
      "source": [
        "# only trust factor\n",
        "\n",
        "def tgt_pred():\n",
        "    R0 = ru    # rating matrix\n",
        "    max_r = 5.0    # max _rating\n",
        "    # trust relationship\n",
        "    N,M= ru.shape[0],ru.shape[1]     # # n : no of users   m : no of items \n",
        "    K = 10                         #k : latent dimension        (mx10)@(10xn)\n",
        "    lambdaU,lambdaV,lambdaT=0.02, 0.02, 0.01\n",
        "    keys = []\n",
        "\n",
        "    R=sp.dok_matrix((N,M))   # create sparse matrix for user x item\n",
        "    T=sp.dok_matrix((N,N))   # create sparse matrix for trust among users user x user\n",
        "    for i in range(len(R0)):    # no of users\n",
        "        for j in range(len(R0[i])):   # no of items\n",
        "            if R0[i][j]>0:    # if rating is present   \n",
        "                keys.append([i,j])\n",
        "                R[i,j] = R0[i][j]      # normalise the rating matrix     R is the new normalised rating matrix which will be used *******\n",
        "\n",
        "    \n",
        "    # get trust factor between users trust_uv(user x user)  and sim_uv(user x user) t_uv(user x user)\n",
        "    df = pd.DataFrame(tgt_pers) \n",
        "    df.columns = ['open', 'cons', 'extra', 'agree', 'neuro']\n",
        "    df['userId'] = src.userId\n",
        "    df2 = df.groupby(by='userId').mean().reset_index()\n",
        "    df2 = np.array(df2[[\"open\", 'cons', 'extra', 'agree', 'neuro']])\n",
        "    trust_uv = 1 - spt.distance.cdist(df2, df2, 'cosine')\n",
        "    trust_uv = np.nan_to_num(trust_uv)\n",
        "    trust_uv_s = trust_uv.copy()\n",
        "    trust_uv = pd.DataFrame(trust_uv > 0.7)\n",
        "    trust_uv = trust_uv.replace(True, 1)\n",
        "    trust_uv = trust_uv.replace(False, 0)\n",
        "    trust_uv = np.array(trust_uv)\n",
        "    tknn = []\n",
        "    for j in range(len(trust_uv)):\n",
        "      row = []\n",
        "      for k in range(len(trust_uv[j])):\n",
        "        if trust_uv[j][k] == 1:\n",
        "          row.append(k)\n",
        "      tknn.append(row)\n",
        "    tknn = np.array(tknn)\n",
        "    sim_uv = 1 - spt.distance.cdist(ru_m, ru_m, 'cosine')\n",
        "    sim_uv = np.nan_to_num(sim_uv)\n",
        "    #t_uv = (np.add(trust_uv, sim_uv)/2)\n",
        "    t_uv = trust_uv\n",
        "    print(t_uv.shape)\n",
        "\n",
        "\n",
        "    max_value = max(t_uv.flatten())\n",
        "    T_df = pd.DataFrame(t_uv)\n",
        "    T_df = T_df >= 0.7 * max_value\n",
        "    T_df = T_df.replace(True, 1)\n",
        "    T_df = T_df.replace(False, 0)\n",
        "    T_df = np.array(T_df)\n",
        "    T = sp.csr_matrix(T_df)      # fill up trust matrix from user realtionships   *******\n",
        "    keys = np.array(keys)        # get the keys\n",
        "    R,T = R.tocsr(),T.tocsr()\n",
        "    print(\"entering RSTE\")\n",
        "    RSTE(R,T,N,M,K,lambdaU,lambdaV,lambdaT,R,keys[:, 0],keys[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmo0lJwtxutO"
      },
      "outputs": [],
      "source": [
        "without log 3.4493965705680547     0.10282609135755258\n",
        "\n",
        "log         3.4503007416729585     0.11241594781533616\n",
        "\n",
        "yakchi pacis    3.4490502620041434   0.08701762665204069"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba0Oh-KLmwNZ"
      },
      "outputs": [],
      "source": [
        "# @params: \n",
        "# trust threshold (0.7)\n",
        "# trust factor lambda (0.5)\n",
        "# lambdaU,lambdaV,lambdaT (0.02, 0.02, 0.1)\n",
        "# no of iterations (100)\n",
        "# no of latent features K (10)\n",
        "# train test split\n",
        "\n",
        "# metrics\n",
        "# MAE\n",
        "# RMSE\n",
        "\n",
        "# benchmark\n",
        "# yakchi pacis\n",
        "# tobias umap\n",
        "# p2mf cdrup\n",
        "\n",
        "# variations\n",
        "# numerical personality values (0,1)\n",
        "# binary personality values [0 and 1]\n",
        "# with demography dbscan clustering\n",
        "  # ensemble through average voting\n",
        "  # ensemble through plurality voting \n",
        "# without clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tuning"
      ],
      "metadata": {
        "id": "-IQQ0uzt-EuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = [0.01]\n",
        "K = [10]\n",
        "lambda_p = [0.01]\n",
        "lambda_t = [0.1]\n",
        "thres = [0.8]"
      ],
      "metadata": {
        "id": "kNMb8ITA-Dsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l, lp, lt, th, k = 0.01, 0.01, 0.1, 0.8, 10"
      ],
      "metadata": {
        "id": "t5Jaf9NOQIsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "     model = tgt_pred(l, lp, lt, th, k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i5OST7i-Lk9",
        "outputId": "ebe36e5c-d977-468a-b418-165d98f4f26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2343, 2343)\n",
            "entring rste\n",
            "0.01 0.01 0.1 0.8 10\n",
            "0 mae 0.04761326546100986 rmse 1.6493718978557383 2.679325819015503\n",
            "5 mae 0.04728160692871591 rmse 1.637882909281794 2.63616943359375\n",
            "10 mae 0.04601757319156814 rmse 1.5940954961773437 2.670991897583008\n",
            "=================RESULT=======================\n",
            "K:10,lambdaU:0.01, lambdaV:0.01,lambdaT:0.1\n",
            "rmse 20.023104470335646\n",
            "mae 3.7879757522079087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "xurBooAK_zWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rja_cLdHMXbw"
      ],
      "name": "Social Trust Ensemble.ipynb",
      "provenance": [],
      "mount_file_id": "122nkML_W6apGijganyKiU2iL4ieMaipu",
      "authorship_tag": "ABX9TyN90s19ve5AWFHyKoPEp8pT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}