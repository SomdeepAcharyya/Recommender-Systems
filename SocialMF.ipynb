{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomdeepAcharyya/Recommender-Systems/blob/main/SocialMF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08IHqa4pgM8h"
      },
      "outputs": [],
      "source": [
        "# SocialMF\n",
        "# A Matrix Factorization Technique with Trust Propagation for Recommendation in Social Networks\n",
        "# M Jamali, M Ester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQGxLXcXklX6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numba as nb\n",
        "import scipy.sparse as sp\n",
        "import time\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import scipy.spatial as spt\n",
        "import statistics\n",
        "import math\n",
        "import json\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from random import sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnPMs77gKxJC"
      },
      "outputs": [],
      "source": [
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY,p.OPT.MENTION,p.OPT.HASHTAG, p.OPT.ESCAPE_CHAR, p.OPT.RESERVED)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from textblob import Word\n",
        "import re\n",
        "punctuation = re.compile(r'[-.?&!,:;()|0-9]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S7t-idOyXui"
      },
      "outputs": [],
      "source": [
        "def socialMF(R,S,N,M,K,lambdaU,lambdaV,lambdaT,R_test,ul,il):\n",
        "    def sigmoid(z):\n",
        "        return 1 + ((5.0 - 1.0) / (1+np.exp(-z)))\n",
        "    def dsigmoid(z):\n",
        "        return (5.0 - 1.0) * np.exp(-z)/np.power((1+np.exp(-z)),2)\n",
        "    def rmse(U,V,R):\n",
        "        keylist = []\n",
        "        dok_keys = np.array(R.todok().keys()).T.reshape([1,1])[0][0]\n",
        "        for k in dok_keys:\n",
        "          row = []\n",
        "          for l in k:\n",
        "            row.append(l)\n",
        "          keylist.append(row) \n",
        "        keylist = np.array(keylist)\n",
        "        utl = keylist[:, 0]\n",
        "        itl = keylist[:, 1]\n",
        "        error = (get_csrmat(sigmoid(U.dot(V.T)),utl,itl)-R).power(2).sum()/R.nnz\n",
        "        return 5*np.sqrt(error)\n",
        "    def mae(U,V,R):\n",
        "        keylist = []\n",
        "        dok_keys = np.array(R.todok().keys()).T.reshape([1,1])[0][0]\n",
        "        for k in dok_keys:\n",
        "          row = []\n",
        "          for l in k:\n",
        "            row.append(l)\n",
        "          keylist.append(row) \n",
        "        keylist = np.array(keylist)\n",
        "        utl = keylist[:, 0]\n",
        "        itl = keylist[:, 1]\n",
        "        error = abs(get_csrmat(sigmoid(U.dot(V.T)),utl,itl)-R).sum()/R.nnz\n",
        "        return error\n",
        "    def get_csrmat(mat,ul,il):\n",
        "        indx = ul*mat.shape[1]+il\n",
        "        return sp.csr_matrix((np.take(np.array(mat),indx),(ul,il)),shape=(N,M))\n",
        "    def costL(U,V):\n",
        "        tmp = U.dot(V.T)\n",
        "        Rx = get_csrmat(sigmoid(tmp),ul,il)\n",
        "        cost = 0.5*((R - Rx).power(2)).sum()+0.5*lambdaU*np.linalg.norm(U)**2+0.5*lambdaV*np.linalg.norm(V)**2\n",
        "        cost += 0.5*lambdaT*np.power(U-S.dot(U),2).sum()\n",
        "        return cost\n",
        "    def gradient(U,V):\n",
        "        dU = np.zeros(U.shape)\n",
        "        dV = np.zeros(V.shape)\n",
        "        dU = lambdaU*U\n",
        "        tmp = U.dot(V.T)\n",
        "        Rv = get_csrmat(dsigmoid(tmp),ul,il)\n",
        "        Rx = get_csrmat(sigmoid(tmp),ul,il)\n",
        "        dU += Rv.multiply((Rx-R)).dot(V)\n",
        "        dU += lambdaT*(U-S.dot(U))-lambdaT*S.T.dot((U-S.dot(U)))\n",
        "        dV = lambdaV*V\n",
        "        dV += (Rv.multiply((Rx-R))).T.dot(U)\n",
        "        # print dU,dV\n",
        "        if np.max(dU)>1:\n",
        "            dU = dU/np.max(dU)\n",
        "        if np.max(dV)>1:\n",
        "            dV = dV/np.max(dV)\n",
        "        return dU,dV\n",
        "\n",
        "\n",
        "    def train(U,V):\n",
        "        res=[]\n",
        "        steps=10\n",
        "        rate = 1e-2\n",
        "        pregradU = 0\n",
        "        pregradV = 0\n",
        "        tol=1e-6\n",
        "        momentum = 0.9\n",
        "        stage = max(steps/100 , 1)\n",
        "        for step in range(steps):\n",
        "            start = time.time()\n",
        "            dU,dV = gradient(U,V)\n",
        "            dU = dU + momentum*pregradU\n",
        "            dV = dV + momentum*pregradV\n",
        "            pregradU = dU\n",
        "            pregradV = dV\n",
        "            if not step%stage and rate>0.001:\n",
        "                rate = 0.95*rate\n",
        "            U -= rate * dU\n",
        "            V -= rate * dV\n",
        "            e = costL(U,V) / (len(U) * len(V))\n",
        "            res.append(e)\n",
        "            if not step%stage:\n",
        "                print(step,e,time.time() - start)\n",
        "                #print(\"RMSE\",rmse(U,V,R), \"MAE\",mae(U,V,R))\n",
        "                e1 = e\n",
        "            if step>150: #or abs(sum(res[-3:])-sum(res[-13:-10]))<tol:\n",
        "                print(\"====================\")\n",
        "                print(\"stop in %d step\"%(step))\n",
        "                print(\"error is \",e)\n",
        "                print(\"====================\")\n",
        "                break\n",
        "        return U, V\n",
        "\n",
        "\n",
        "    U = np.random.normal(0,0.01,size=(N,K))\n",
        "    V = np.random.normal(0,0.01,size=(M,K))\n",
        "    start = time.time()\n",
        "    U,V = train(U,V)\n",
        "    print(\"=================RESULT=======================\")\n",
        "    print('K:%d,lambdaU:%s, lambdaV:%s,lambdaT:%s' \\\n",
        "            %(K,lambdaU,lambdaV,lambdaT))\n",
        "    print(\"rmse\",rmse(U,V,R_test))\n",
        "    print(\"mae\",mae(U,V,R_test))\n",
        "    print(\"time\",time.time() - start)\n",
        "    print(\"========================================\")\n",
        "    #print(\"mae recal\", mean_absolute_error((U@V.T).toarray(), R_test))\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_distance(A,A1):\n",
        "  similarity = np.dot(A, A1.T)\n",
        "  square_mag = np.diag(similarity)\n",
        "  inv_square_mag = 1 / square_mag\n",
        "  inv_square_mag[np.isinf(inv_square_mag)] = 0\n",
        "  inv_mag = np.sqrt(inv_square_mag)\n",
        "  cosine = similarity * inv_mag\n",
        "  cosine = cosine.T * inv_mag\n",
        "  return cosine"
      ],
      "metadata": {
        "id": "BxVJNZOLxc-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q5JrmZxLhf-"
      },
      "source": [
        "## examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqMsBaIXydqt"
      },
      "outputs": [],
      "source": [
        "def t_yelp(limitu,limiti):\n",
        "    #data from: http://www.trustlet.org/wiki/Epinions_datasets\n",
        "    def getdata():\n",
        "        N,M = limitu,limiti\n",
        "        max_r = 5.0\n",
        "        cNum = 8\n",
        "        T=sp.dok_matrix((N,N))\n",
        "        print('get T')\n",
        "        for line in open('./yelp_data/users.txt','r'):\n",
        "            u = int(line.split(':')[0])\n",
        "            uf = line.split(':')[1][1:-1].split(',')\n",
        "            if len(uf)>1:\n",
        "                for x in line.split(':')[1][1:-1].split(',')[:-1]:\n",
        "                    v = int(x)\n",
        "                    if u<limitu and v<limitu:\n",
        "                        T[u,v] = 1.0\n",
        "        T = T.tocsr()\n",
        "        print('get R_test')\n",
        "        utl,itl,rtl = [],[],[]\n",
        "        for line in open('./yelp_data/ratings-test.txt','r'):\n",
        "            u,i,r = [int(x) for x in line.split('::')[:3]]\n",
        "            if u<limitu and i<limiti:\n",
        "                utl.append(u)\n",
        "                itl.append(i)\n",
        "                rtl.append(r/5.0)\n",
        "        utl,itl = np.array(utl),np.array(itl)\n",
        "        R_test = sp.csr_matrix((rtl,(utl,itl)),shape=(N,M))\n",
        "        print('get R')\n",
        "        ul,il,rl = [],[],[]\n",
        "        for line in open('./yelp_data/ratings-train.txt','r'):\n",
        "            u,i,r = [int(x) for x in line.split('::')[:3]]\n",
        "            if u<limitu and i<limiti:\n",
        "                ul.append(u)\n",
        "                il.append(i)\n",
        "                rl.append(r/5.0)\n",
        "        ul,il = np.array(ul),np.array(il)\n",
        "        R = sp.csr_matrix((rl,(ul,il)),shape=(N,M))\n",
        "        # print \"get Circle\"\n",
        "        # C = [[] for i in range(cNum)]\n",
        "        # for line in open('./yelp_data/items-class.txt','r'):\n",
        "        #     i,ci = [int(x) for x in line.split(' ')]\n",
        "        #     if i<limit:\n",
        "        #         C[ci].append(i)\n",
        "        return R,T,N,M,R_test,ul,il\n",
        "    R,T,N,M,R_test,ul,il = getdata()\n",
        "\n",
        "    lambdaU,lambdaV,lambdaT,K = 0.01, 0.01, 0.3, 5\n",
        "    socialMF(R,T,N,M,K,lambdaU,lambdaV,lambdaT,R_test,ul,il)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAzXdWOczcUC"
      },
      "outputs": [],
      "source": [
        "def t_toy():\n",
        "    R0 = [\n",
        "         [5,3,0,1],\n",
        "         [4,0,0,1],\n",
        "         [1,1,0,5],\n",
        "         [1,0,0,4],\n",
        "         [0,1,5,4],\n",
        "        ]     # rating matrix\n",
        "    max_r = 5.0    # max _rating\n",
        "    T0 = [[3,2],[1,3,4],[2],[1,5],[3]]     # trust relationship\n",
        "    N,M,K=5,4,4     # n : no of users   m : no of items k : latent dimension        (5x4)@(4x4)\n",
        "    lambdaU,lambdaV,lambdaT=0.02, 0.02, 0.1\n",
        "\n",
        "    R=sp.dok_matrix((N,M))   # create sparse matrix for user x item\n",
        "    T=sp.dok_matrix((N,N))   # create sparse matrix for trust among users user x user\n",
        "    for i in range(len(R0)):    # no of users\n",
        "        for j in range(len(R0[0])):   # no of items\n",
        "            if R0[i][j]>0:    # if rating is present   \n",
        "                R[i,j] = 1.0 * R0[i][j] / max_r      # normalise the rating matrix     R is the new normalised rating matrix which will be used *******\n",
        "    print(R.toarray())\n",
        "    for i in range(len(T0)):    # no of users\n",
        "        for j in T0[i]:         # no of trusted users of user i\n",
        "            T[i,j-1]=1.0        # fill up trust matrix from user realtionships   *******\n",
        "    print(T.toarray())\n",
        "    keys = np.array(R.keys()).T\n",
        "    print(np.array(keys[0]))\n",
        "    R,T = R.tocsr(),T.tocsr()\n",
        "    print(R.shape, T.shape)\n",
        "    socialMF(R,T,N,M,K,lambdaU,lambdaV,lambdaT,R,np.array(keys[0]),np.array(keys[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMRE3WCm6hsQ"
      },
      "outputs": [],
      "source": [
        "def t_toy_correct():\n",
        "    R0 = [\n",
        "         [5,3,0,1],\n",
        "         [4,0,0,1],\n",
        "         [1,1,0,5],\n",
        "         [1,0,0,4],\n",
        "         [0,1,5,4],\n",
        "        ]     # rating matrix\n",
        "    max_r = 5.0    # max _rating\n",
        "    T0 = [[3,2],[1,3,4],[2],[1,5],[3]]     # trust relationship\n",
        "    N,M,K=5,4,4     # n : no of users   m : no of items k : latent dimension        (5x4)@(4x4)\n",
        "    lambdaU,lambdaV,lambdaT=0.02, 0.02, 0.1\n",
        "    keys = []\n",
        "\n",
        "    R=sp.dok_matrix((N,M))   # create sparse matrix for user x item\n",
        "    T=sp.dok_matrix((N,N))   # create sparse matrix for trust among users user x user\n",
        "    for i in range(len(R0)):    # no of users\n",
        "        for j in range(len(R0[0])):   # no of items\n",
        "            if R0[i][j]>0:    # if rating is present   \n",
        "                keys.append([i,j])\n",
        "                R[i,j] = 1.0 * R0[i][j] / max_r      # normalise the rating matrix     R is the new normalised rating matrix which will be used *******\n",
        "    for i in range(len(T0)):    # no of users\n",
        "        for j in T0[i]:         # no of trusted users of user i\n",
        "            T[i, j-1]=1.0        # fill up trust matrix from user realtionships   *******\n",
        "    keys = np.array(keys)        # get the keys\n",
        "    R,T = R.tocsr(),T.tocsr()\n",
        "    socialMF(R,T,N,M,K,lambdaU,lambdaV,lambdaT,R,keys[:, 0],keys[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMCWwc8RzpxC"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "     t_toy_correct()\n",
        "     # t_yelp(1000,20000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qoh5U_kVLnio"
      },
      "source": [
        "# target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpFHYiuBWgC7"
      },
      "outputs": [],
      "source": [
        "# Tripadvisor review Dataset\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/tripadvisor_reviews_with_country.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  tr = pd.read_csv(infile)\n",
        "tr = tr.rename(columns={\"username\":\"userId\", \"taObject\":\"itemId\"})\n",
        "arr = tr[['open', 'cons', 'extra', 'agree', 'neuro', 'userId', 'itemId', 'rating']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoYNhrun4QC9"
      },
      "outputs": [],
      "source": [
        "# amazon review dataset magazines csv\n",
        "path1 = r'/content/drive/MyDrive/Per_CD_RS/pers_fashion_filtered.csv'\n",
        "path2 = r'/content/drive/MyDrive/Per_CD_RS/Aaamzon_fashion_ru_tf.csv'\n",
        "\n",
        "with open(path1, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  df = pd.read_csv(infile)\n",
        "arr = np.array(df[['0', '1', '2', '3', '4']])\n",
        "\n",
        "with open(path2, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  ru_tf_df = pd.read_csv(infile)\n",
        "ru_tf_df = ru_tf_df.drop(columns=['Unnamed: 0'])\n",
        "ru_tf = np.array(ru_tf_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# amazon review dataset movies json\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/Amazon_fashion_filtered.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  az = pd.read_csv(infile)\n",
        "#am = am.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})"
      ],
      "metadata": {
        "id": "NrpayR6tMfBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOVLqNxmDUR2"
      },
      "outputs": [],
      "source": [
        "input_file =  r'/content/drive/MyDrive/Per_CD_RS/Amazon_Text_Video_Games.json'\n",
        "with open(input_file) as f:\n",
        "    lines = f.read().splitlines()\n",
        "df_inter = pd.DataFrame(lines)\n",
        "df_inter.columns = ['json_element']\n",
        "df_inter['json_element'].apply(json.loads)\n",
        "az = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
        "\n",
        "arr1 = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})\n",
        "x = pd.DataFrame(arr1.userId.value_counts()).reset_index()\n",
        "y = x[x['userId']>=5]\n",
        "arr2 = pd.DataFrame(y)\n",
        "arr2 = arr2.rename(columns={\"userId\":\"count\",\"index\":\"userId\"})\n",
        "df = pd.merge(arr1, arr2, on='userId', how='inner')\n",
        "df = df[df['rating']<6]\n",
        "df = df[df['rating']>0]\n",
        "x = pd.DataFrame(df.itemId.value_counts()).reset_index()\n",
        "y = x[x['itemId']>=5]\n",
        "arr3 = pd.DataFrame(y)\n",
        "arr3 = arr3.rename(columns={\"itemId\":\"count\",\"index\":\"itemId\"})\n",
        "df2 = pd.merge(df, arr3, on='itemId', how='inner')\n",
        "az = df2\n",
        "\n",
        "tgt = az\n",
        "\n",
        "tgt['processed_text'] = \"\"\n",
        "tgt['reviewText'].fillna(\" \")\n",
        "array_text = []\n",
        "for i in range(len(tgt)):\n",
        "  x = tgt['reviewText'][i]\n",
        "  word_tokens = word_tokenize(x) if type(x) != float else  \" \"\n",
        "  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  lemma_words = []\n",
        "  for words in filtered_sentence:\n",
        "      word = Word(words).lemmatize()\n",
        "      lemma_words.append(word)\n",
        "  punc_words = []\n",
        "  for words in lemma_words:\n",
        "      word =  punctuation.sub(\"\", words)\n",
        "      if len(word) > 0:\n",
        "        punc_words.append(word.lower())\n",
        "  line = \"\"\n",
        "  for i in punc_words:\n",
        "    line = line + \" \" + i.lower()\n",
        "  array_text.append(line)\n",
        "\n",
        "tgt['processed_text'] = array_text\n",
        "az = tgt[['rating', 'userId', 'itemId', 'reviewText', 'processed_text']]\n",
        "az = az.drop_duplicates(subset=None, keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(arr.shape, az.shape)\n",
        "az = az[0:len(arr)]\n",
        "print(arr.shape, az.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f36zI8UonE3j",
        "outputId": "717f519f-8e04-4d68-8d72-32596681a88e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7841, 5) (7891, 6)\n",
            "(7841, 5) (7841, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqTOfAhLJH7i"
      },
      "outputs": [],
      "source": [
        "# amazon review dataset movies json\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/Amazon_fashion_filtered.csv'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  az = pd.read_csv(infile)\n",
        "az = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LnDQNXHguUs"
      },
      "outputs": [],
      "source": [
        "# amazon review dataset magazines csv\n",
        "path = r'/content/drive/MyDrive/Per_CD_RS/Amazon_Text_Movies_and_TV.json'\n",
        "with open(path, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "  az = pd.read_json(infile, lines=True, nrows=12000)\n",
        "az = az.rename(columns={\"reviewerID\":\"userId\", \"asin\":\"itemId\", \"overall\":\"rating\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37yzIrr3hB2V"
      },
      "outputs": [],
      "source": [
        "src = az[['userId', 'itemId', 'rating']]\n",
        "arr = pd.DataFrame(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pldk1H9HLWMl"
      },
      "outputs": [],
      "source": [
        "arr['userId'] = az['userId']\n",
        "arr['itemId'] = np.array(az['itemId'])\n",
        "arr['rating'] = az['rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwnmcYvDhEoR"
      },
      "outputs": [],
      "source": [
        "arr.columns = ['open', 'cons', 'extra', 'agree', 'neuro', 'userId', 'itemId', 'rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoEu4fwZhLL6"
      },
      "outputs": [],
      "source": [
        "# rating matrix wrt user u\n",
        "ru = arr.pivot_table(index='userId',columns='itemId',values='rating')\n",
        "ru = ru.fillna(0)\n",
        "ru_m = ru > 0\n",
        "ru_m = ru_m.replace(True, 1)\n",
        "ru_m = ru_m.replace(False, 0)\n",
        "ru = np.array(ru)\n",
        "ru_m = np.array(ru_m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8UipsLlMESh"
      },
      "outputs": [],
      "source": [
        "ru = arr.groupby(['userId', 'itemId'])['rating'].sum().unstack()\n",
        "ru = pd.DataFrame(ru)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ru.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "821qASr56byy",
        "outputId": "28d0af47-3e60-4710-f377-e21d950a097b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40639, 16794)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set1  = ru.iloc[: , :int(ru.shape[1]/10)]\n",
        "set1 = set1.replace(np.nan, 0)"
      ],
      "metadata": {
        "id": "c_ZPU3IO-9eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,3):\n",
        "  set2 = ru.iloc[: , i*int(ru.shape[1]/10):(i+1)*int(ru.shape[1]/10)]\n",
        "  set2 = set2.replace(np.nan, 0)\n",
        "  set1 = pd.concat([set1, set2], axis=1)\n",
        "  print(set2.shape, set1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JunMaXAV9bhe",
        "outputId": "9bb3612d-2d92-4281-939e-8824f345c33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40639, 1679) (40639, 3358)\n",
            "(40639, 1679) (40639, 5037)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set1  = ru.iloc[: , :int(ru.shape[1]/10)]\n",
        "set1 = set1.replace(np.nan, 0)\n",
        "set1"
      ],
      "metadata": {
        "id": "NDukqJ9Q30cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set1"
      ],
      "metadata": {
        "id": "EXRMbp7Y7-Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set2  = ru.iloc[: , int(ru.shape[1]/10): 2*int(ru.shape[1]/10)]\n",
        "set2 = set2.replace(np.nan, 0)\n",
        "set2"
      ],
      "metadata": {
        "id": "dcKCbVqQ7nAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([set1, set2], axis=1)"
      ],
      "metadata": {
        "id": "R9GgTDuK8IPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mprzddQjhRfs"
      },
      "outputs": [],
      "source": [
        "# with only trust and rating use this\n",
        "\n",
        "def tgt_pred(l, k, a, lp, t, th):\n",
        "\n",
        "    train_size = 0.8\n",
        "    lr, k, alpha, lambda_p, lambda_t, thr = l, k, a, lp, t, th\n",
        "    df_copy = arr.copy()\n",
        "    train_set = sample(df_copy, frac=train_size).reset_index()       # train set\n",
        "    user_features_train = np.array(train_set[['open', 'cons', 'extra', 'agree', 'neuro']].fillna(0))\n",
        "    test_set = df_copy.drop(train_set.index).reset_index()\n",
        "    user_features_test = np.array(test_set[['open', 'cons', 'extra', 'agree', 'neuro']].fillna(0))\n",
        "\n",
        "    df = pd.DataFrame(user_features_train) \n",
        "    df.columns = ['open', 'cons', 'extra', 'agree', 'neuro']\n",
        "    df['userId'] = train_set.userId\n",
        "    df2 = df.groupby(by='userId').mean().reset_index()\n",
        "    df3 = np.array(df2[[\"open\", 'cons', 'extra', 'agree', 'neuro']])\n",
        "    \n",
        "\n",
        "    #ru = train_set.pivot_table(index='userId',columns='itemId',values='rating')\n",
        "    ru = train_set.groupby(['userId', 'itemId'])['rating'].sum().unstack()\n",
        "    ru = pd.DataFrame(ru)\n",
        "    set1 = ru.iloc[: , :int(ru.shape[1]/10)]\n",
        "    set1 = set1.replace(np.nan, 0)\n",
        "    #for i in range(1,2):\n",
        "    # set2 = ru.iloc[: , i*int(ru.shape[1]/10):(i+1)*int(ru.shape[1]/10)]\n",
        "    #  set2 = set2.replace(np.nan, 0)\n",
        "    #  set1 = pd.concat([set1, set2], axis=1)\n",
        "    ru = set1\n",
        "    #ru = ru.fillna(0)\n",
        "    ru_m = ru > 0\n",
        "    ru_m = ru_m.replace(True, 1)\n",
        "    ru_m = ru_m.replace(False, 0)\n",
        "    ru = np.array(ru)\n",
        "    ru_m = np.array(ru_m)\n",
        "\n",
        "    R0 = ru    # rating matrix\n",
        "    max_r = 5.0    # max _rating   \n",
        "    N,M= ru.shape[0],ru.shape[1]     # # n : no of users   m : no of items \n",
        "    K = k                       #k : latent dimension        (mx10)@(10xn)\n",
        "    lambdaU,lambdaV,lambdaT= lambda_p, lambda_p, lambda_t\n",
        "    keys = []\n",
        "\n",
        "    R=sp.dok_matrix((N,M))   # create sparse matrix for user x item\n",
        "    T=sp.dok_matrix((N,N))   # create sparse matrix for trust among users user x user\n",
        "    for i in range(len(R0)):    # no of users\n",
        "        for j in range(len(R0[i])):   # no of items\n",
        "            if R0[i][j]>0:    # if rating is present   \n",
        "                keys.append([i,j])\n",
        "                R[i,j] = R0[i][j]     \n",
        "\n",
        "    dft = pd.DataFrame(user_features_test) \n",
        "    dft.columns = ['open', 'cons', 'extra', 'agree', 'neuro']\n",
        "    dft['userId'] = test_set.userId\n",
        "    df2t = dft.groupby(by='userId').mean().reset_index()\n",
        "    df3t = np.array(df2t[[\"open\", 'cons', 'extra', 'agree', 'neuro']])\n",
        "\n",
        "    rut = test_set.pivot_table(index='userId',columns='itemId',values='rating')\n",
        "    rut = test_set.groupby(['userId', 'itemId'])['rating'].sum().unstack()\n",
        "    rut = pd.DataFrame(rut)\n",
        "    rut = rut.fillna(0)\n",
        "    ru_mt = rut > 0\n",
        "    ru_mt = ru_mt.replace(True, 1)\n",
        "    ru_mt = ru_mt.replace(False, 0)\n",
        "    rut = np.array(rut)\n",
        "    ru_mt = np.array(ru_mt)\n",
        "\n",
        "    R0t = rut    # rating matrix\n",
        "    max_rt = 5.0    # max _rating   \n",
        "    Nt,Mt= rut.shape[0],rut.shape[1]     # # n : no of users   m : no of items \n",
        "    K = k                        #k : latent dimension        (mx10)@(10xn)\n",
        "    lambdaU,lambdaV,lambdaT=0.02, 0.02, 0.1\n",
        "    keyst = []\n",
        "\n",
        "    Rt=sp.dok_matrix((Nt,Mt))   # create sparse matrix for user x item\n",
        "    for i in range(len(R0t)):    # no of users\n",
        "        for j in range(len(R0t[i])):   # no of items\n",
        "            if R0t[i][j]>0:    # if rating is present   \n",
        "                keyst.append([i,j])\n",
        "                Rt[i,j] = R0t[i][j]      \n",
        "\n",
        "    \n",
        "    # get trust factor between users trust_uv(user x user)  and sim_uv(user x user) t_uv(user x user)\n",
        "\n",
        "\n",
        "    print(\"done0\")\n",
        "    trust_uv = 1 - calc_distance(df3, df3)\n",
        "    #trust_uv = 1 - spt.distance.cdist(df3, df3, 'cosine')\n",
        "    print(\"done1\")\n",
        "    trust_uv = np.nan_to_num(trust_uv)\n",
        "    print(\"done2\")\n",
        "    trust_uv_s = trust_uv.copy()\n",
        "    print(\"done3\")\n",
        "    trust_uv = pd.DataFrame(trust_uv >= thr)\n",
        "    print(\"done4\")\n",
        "    trust_uv = trust_uv.replace(True, 1)\n",
        "    trust_uv = trust_uv.replace(False, 0)\n",
        "    print(\"done5\")\n",
        "    trust_uv = np.array(trust_uv)\n",
        "    tknn = []\n",
        "    for j in range(len(trust_uv)):\n",
        "      row = []\n",
        "      for m in range(len(trust_uv[j])):\n",
        "        if trust_uv[j][m] == 1:\n",
        "          row.append(m)\n",
        "      tknn.append(row)\n",
        "    tknn = np.array(tknn)\n",
        "    sim_uv = 1- calc_distance(ru, ru)\n",
        "    #sim_uv = 1 - spt.distance.cdist(ru, ru, 'cosine')\n",
        "    sim_uv = np.nan_to_num(sim_uv)\n",
        "    ru_tf2 = ru_tf[0:trust_uv.shape[0]]\n",
        "    ru_tf2 = ru_tf[:,:len(trust_uv)]\n",
        "    print(sim_uv.shape, trust_uv.shape, R.shape)\n",
        "    #t_uv = np.add(alpha * trust_uv_s, (1-alpha)* sim_uv)\n",
        "    t_uv = np.multiply(np.add(alpha * trust_uv_s, (1-alpha)* sim_uv), ru_tf)\n",
        "    t_uv = trust_uv_s\n",
        "\n",
        "    max_value = max(t_uv.flatten())\n",
        "    min_value = min(t_uv.flatten())\n",
        "    T_df = pd.DataFrame(t_uv)\n",
        "    T_df = T_df >=  np.subtract(T_df, min_value) /(max_value - min_value)   # thr is threshold value\n",
        "    T_df = T_df.replace(True, 1)\n",
        "    T_df = T_df.replace(False, 0)\n",
        "    T_df = np.array(T_df)\n",
        "    T = sp.csr_matrix(T_df)\n",
        "           # fill up trust matrix from user realtionships   *******\n",
        "    keys = np.array(keys)        # get the keys\n",
        "    R,T = R.tocsr(),T.tocsr()\n",
        "    print(\"entring smf\")\n",
        "    print(l, k, a, lp, t, th)\n",
        "    socialMF(R,T,N,M,K,lambdaU,lambdaV,lambdaT,Rt,keys[:, 0],keys[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQSWNzk40RTL"
      },
      "outputs": [],
      "source": [
        "lr = [0.0001]\n",
        "K = [10]\n",
        "alpha = [0.3]\n",
        "lambda_p = [0.04]\n",
        "lambda_t = [0.1]\n",
        "thres = [0.8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4n4KI1_VxXT"
      },
      "outputs": [],
      "source": [
        "# thresh 0.7, 0.8*, \n",
        "# lambda_t 0.1*, 0.5, 0.8\n",
        "# lambda_p 0.01\n",
        "# alpha = 0.5\n",
        "\n",
        "# run till 30 k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng3F_bKdaLDn"
      },
      "outputs": [],
      "source": [
        "l, k, a, lp, lt, th = 0.0001, 20, 0.5, 0.03, 0.8, 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW8lXBJal7Se",
        "outputId": "f521e008-f77e-4b4c-87a9-6b1f8884242a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done0\n",
            "done1\n",
            "done2\n",
            "done3\n",
            "done4\n",
            "done5\n",
            "(2344, 2344) (2344, 2344) (2344, 44)\n",
            "entring smf\n",
            "0.0001 20 0.5 0.03 0.8 0.8\n",
            "0 mae 0.02305873117024209 rmse 0.7843752572910565 0.020981311798095703\n",
            "1 mae 0.02307682992193197 rmse 0.7849909118519508 0.030782699584960938\n",
            "2 mae 0.0231272568709124 rmse 0.7867062556316629 0.020181655883789062\n",
            "3 mae 0.023126990225893273 rmse 0.7866971853253247 0.01968669891357422\n",
            "4 mae 0.023081741371106922 rmse 0.7851579817215735 0.019634485244750977\n",
            "5 mae 0.023039178111184472 rmse 0.7837101324142403 0.019206523895263672\n",
            "6 mae 0.023055098298457363 rmse 0.784251680034363 0.020754337310791016\n",
            "7 mae 0.023075952277290264 rmse 0.7849610575318425 0.019543886184692383\n",
            "8 mae 0.023050867368834774 rmse 0.7841077589969565 0.02220749855041504\n",
            "9 mae 0.023004290810543 rmse 0.7825233916861919 0.019937515258789062\n",
            "=================RESULT=======================\n",
            "K:20,lambdaU:0.03, lambdaV:0.03,lambdaT:0.8\n",
            "rmse 10.332567816053073\n",
            "mae 1.743289557450823\n",
            "time 0.2563180923461914\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "   for i in range(5): \n",
        "    model = tgt_pred(l, k, a, lp, lt, th)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhKOnLlqlOQY"
      },
      "outputs": [],
      "source": [
        "# @params: \n",
        "# trust threshold (0.7) t  ***\n",
        "# mixing factor alpha (0.5)   ***\n",
        "# lambdaU,lambdaV,lambdaT (0.02, 0.02, 0.1)\n",
        "# no of iterations (100) +++\n",
        "# no of latent features K (10) ***\n",
        "# train test split  \n",
        "# learning rate\n",
        "\n",
        "# metrics\n",
        "# MAE\n",
        "# RMSE\n",
        "\n",
        "# benchmark\n",
        "# yakchi pacis\n",
        "# tobias umap\n",
        "# p2mf cdrup\n",
        "\n",
        "# variations\n",
        "# numerical personality values (0,1)\n",
        "# binary personality values [0 and 1]\n",
        "# with demography dbscan clustering\n",
        "  # ensemble through average voting\n",
        "  # ensemble through plurality voting \n",
        "# without clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ausk0WZewuQp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZU7jUWEwuAF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTkL4-ANL6TQ"
      },
      "outputs": [],
      "source": [
        "# with tf idf of rating\n",
        "\n",
        "def tgt_pred_log():\n",
        "    R0 = ru    # rating matrix\n",
        "    max_r = 5.0    # max _rating\n",
        "    T0 = [[3,2],[1,3,4],[2],[1,5],[3]]     # trust relationship\n",
        "    N,M= ru.shape[0],ru.shape[1]     # # n : no of users   m : no of items \n",
        "    K = 10                         #k : latent dimension        (mx10)@(10xn)\n",
        "    lambdaU,lambdaV,lambdaT=0.02, 0.02, 0.1\n",
        "    keys = []\n",
        "\n",
        "    R=sp.dok_matrix((N,M))   # create sparse matrix for user x item\n",
        "    T=sp.dok_matrix((N,N))   # create sparse matrix for trust among users user x user\n",
        "    for i in range(len(R0)):    # no of users\n",
        "        for j in range(len(R0[i])):   # no of items\n",
        "            if R0[i][j]>0:    # if rating is present   \n",
        "                keys.append([i,j])\n",
        "                R[i,j] = R0[i][j]      # normalise the rating matrix     R is the new normalised rating matrix which will be used *******\n",
        "\n",
        "    \n",
        "    # get trust factor between users trust_uv(user x user)  and sim_uv(user x user) t_uv(user x user)\n",
        "    df = pd.DataFrame(src_pers[0]) \n",
        "    df.columns = ['open', 'cons', 'extra', 'agree', 'neuro']\n",
        "    df['userId'] = src.userId\n",
        "    df2 = df.groupby(by='userId').mean().reset_index()\n",
        "    df2 = np.array(df2[[\"open\", 'cons', 'extra', 'agree', 'neuro']])\n",
        "    trust_uv = 1 - spt.distance.cdist(df2, df2, 'cosine')\n",
        "    trust_uv = np.nan_to_num(trust_uv)\n",
        "    trust_uv_s = trust_uv.copy()\n",
        "    trust_uv = pd.DataFrame(trust_uv > 0.7)\n",
        "    trust_uv = trust_uv.replace(True, 1)\n",
        "    trust_uv = trust_uv.replace(False, 0)\n",
        "    trust_uv = np.array(trust_uv)\n",
        "    tknn = []\n",
        "    for j in range(len(trust_uv)):\n",
        "      row = []\n",
        "      for k in range(len(trust_uv[j])):\n",
        "        if trust_uv[j][k] == 1:\n",
        "          row.append(k)\n",
        "      tknn.append(row)\n",
        "    tknn = np.array(tknn)\n",
        "    sim_uv = 1 - spt.distance.cdist(ru_m, ru_m, 'cosine')\n",
        "    sim_uv = np.nan_to_num(sim_uv)\n",
        "    #t_uv = (np.add(trust_uv, sim_uv)/2)\n",
        "    #t_uv = np.multiply(t_uv, ru_tf)\n",
        "    t_uv = np.add(trust_uv, np.multiply(sim_uv, ru_tf)) / 2\n",
        "    print(t_uv.shape)\n",
        "\n",
        "    max_value = max(t_uv.flatten())\n",
        "    T_df = pd.DataFrame(t_uv)\n",
        "    T_df = T_df >= 0.7 * max_value\n",
        "    T_df = T_df.replace(True, 1)\n",
        "    T_df = T_df.replace(False, 0)\n",
        "    T_df = np.array(T_df)\n",
        "    T = sp.csr_matrix(T_df)\n",
        "           # fill up trust matrix from user realtionships   *******\n",
        "    keys = np.array(keys)        # get the keys\n",
        "    R,T = R.tocsr(),T.tocsr()\n",
        "    print(\"entring smf\")\n",
        "    socialMF(R,T,N,M,K,lambdaU,lambdaV,lambdaT,R,keys[:, 0],keys[:, 1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-q5JrmZxLhf-"
      ],
      "name": "SocialMF.ipynb",
      "provenance": [],
      "mount_file_id": "1sX69V2duCHPbs6AZ0z7xeeq8eihahhap",
      "authorship_tag": "ABX9TyNSqtKZizhsIemkah4RToSX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}